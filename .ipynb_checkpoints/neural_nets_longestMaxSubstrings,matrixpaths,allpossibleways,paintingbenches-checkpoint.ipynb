{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [<__main__.neuron at 0x103f55898>,\n",
       "  <__main__.neuron at 0x103f6c4e0>,\n",
       "  <__main__.neuron at 0x103f6c828>,\n",
       "  <__main__.neuron at 0x103f6c8d0>,\n",
       "  <__main__.neuron at 0x103f6c780>],\n",
       " 1: [<__main__.neuron at 0x103f6c860>,\n",
       "  <__main__.neuron at 0x103f6c6d8>,\n",
       "  <__main__.neuron at 0x103f6c7f0>,\n",
       "  <__main__.neuron at 0x103f6c630>,\n",
       "  <__main__.neuron at 0x103f6c7b8>],\n",
       " 2: [<__main__.neuron at 0x103f6c5c0>,\n",
       "  <__main__.neuron at 0x103f6c710>,\n",
       "  <__main__.neuron at 0x103f6c550>,\n",
       "  <__main__.neuron at 0x103f6c668>,\n",
       "  <__main__.neuron at 0x103f6c518>],\n",
       " 3: [<__main__.neuron at 0x103f6c978>,\n",
       "  <__main__.neuron at 0x103f6c9b0>,\n",
       "  <__main__.neuron at 0x103f6c9e8>,\n",
       "  <__main__.neuron at 0x103f6ca20>,\n",
       "  <__main__.neuron at 0x103f6ca58>],\n",
       " 4: [<__main__.neuron at 0x103f55b00>],\n",
       " 5: [<__main__.neuron at 0x103f6ca90>],\n",
       " 6: [<__main__.neuron at 0x103f6cac8>],\n",
       " 7: [<__main__.neuron at 0x103f6cb00>],\n",
       " 8: [<__main__.neuron at 0x103f6cb38>],\n",
       " 9: []}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#neural networking feed forward using topological sort\n",
    "\n",
    "import random\n",
    "\n",
    "class neuron(object):\n",
    "    def __init__(self, ind, weight, act):\n",
    "        self.index = ind\n",
    "        self.weight = weight\n",
    "        self.act = act\n",
    "\n",
    "        \n",
    "random.seed()\n",
    "        \n",
    "    \n",
    "# network with two layers, one input, one hidden, one output. 6 neurons in total network apart from inputs\n",
    "\n",
    "#fully connected network\n",
    "\n",
    "#inp    neuron1(relu) \n",
    "#inp    neuron2(relu)     output6(sigmoid)\n",
    "#inp    neuron3(relu)\n",
    "#inp    neuron4(relu)\n",
    "#       neuron5(relu)\n",
    "\n",
    "#creating a graph(adjacency list) to represent neural network\n",
    "\n",
    "net = {}\n",
    "inp = [10,10,10,10]\n",
    "actfns = [\"no\",\"no\",\"no\",\"no\",\"relu\",\"relu\",\"relu\",\"relu\",\"relu\",\"sig\"]\n",
    "inputs = [neuron(i,random.randint(1,10),1) for i in range(4)]\n",
    "#hidden_layer_1 = [neuron(i,random.rand(0.1,1),\"relu\") for i in range(4,9)]\n",
    "#layer_2 = [neuron(9, ,\"sig\")]\n",
    "\n",
    "hidden_layer_1 = []\n",
    "for i in inputs:\n",
    "    hidden_layer_1 = []\n",
    "    for j in range(4,9):\n",
    "        r = 0.01*random.random()\n",
    "        hidden_layer_1.append(neuron(j,r,\"relu\"))\n",
    "        \n",
    "    net[i.index] = hidden_layer_1\n",
    "\n",
    "for i in hidden_layer_1:\n",
    "    layer_2 = []\n",
    "    layer_2.append(neuron(9,random.random(),\"sig\"))\n",
    "    net[i.index] = layer_2\n",
    "\n",
    "net[9] = []\n",
    "    \n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output of each neuron: \n",
      "[10, 10, 10, 10]\n",
      "first four: inputs [10, 10, 10, 10]\n",
      "four hidden layer neurons with relu: [0.23400985136269817, 0.15969108703662183, 0.23293216001327016, 0.1483956857765833, 0.20002934540240763]\n",
      "last one after sigmoid:  0.6413899112794987\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 0.23400985136269817,\n",
       " 0.15969108703662183,\n",
       " 0.23293216001327016,\n",
       " 0.1483956857765833,\n",
       " 0.20002934540240763,\n",
       " 0.6413899112794987]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "#using topological sort to do forward propagation and printing output at each neuron \n",
    "\n",
    "def indegree(net):\n",
    "    indeg = [0]*len(net)\n",
    "    for node in net:\n",
    "        for otnode in net:\n",
    "            if node is not otnode:\n",
    "                for neu in net[otnode]:\n",
    "                    if neu.index == node:\n",
    "                        indeg[node]+=1\n",
    "      \n",
    "    return indeg\n",
    "\n",
    "\n",
    "def calculate_output(net, sumn, indeg):\n",
    "    #activation functions\n",
    "    act = {\"relu\": lambda x: max(0,x), \n",
    "           \"sig\": lambda x: 1/(1+math.exp(-x)), \n",
    "           \"no\": lambda x:x,\n",
    "           \"tanh\": lambda x: math.tanh(x), \n",
    "           \"Elu\": lambda x: max(0.01*x,x)\n",
    "          }\n",
    "\n",
    "    queue = [i for i in range(len(indeg)) if indeg[i] == 0]\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    while queue:\n",
    "        \n",
    "        top = queue.pop(0)\n",
    "        \n",
    "        count+=1\n",
    "        sm_top = act[actfns[top]](sumn[top])\n",
    "                                  \n",
    "        for neu in net[top]:\n",
    "            sumn[neu.index] += sm_top*neu.weight\n",
    "            \n",
    "            indeg[neu.index] -= 1\n",
    "            if indeg[neu.index] == 0:\n",
    "                queue.append(neu.index)\n",
    "            \n",
    "    if count != len(net):\n",
    "        print(\"found circle\")\n",
    "        return\n",
    "    \n",
    "    sumn[-1] = act[neu.act](sumn[-1])\n",
    "    return sumn\n",
    "    \n",
    "    \n",
    "    \n",
    "sumn = [0]*len(net)\n",
    "for i in range(len(inp)):\n",
    "    sumn[i] = inp[i]\n",
    "    \n",
    "indeg = indegree(net)\n",
    "su = calculate_output(net, sumn, indeg)\n",
    "print(\"output of each neuron: \")\n",
    "print(su[:4])\n",
    "print(\"first four: inputs\" , su[:4])\n",
    "print(\"four hidden layer neurons with relu:\",su[4:9])\n",
    "print(\"last one after sigmoid: \", su[9])\n",
    "sumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer  0  mean:  -0.0011728342345722778  stds:  0.9820343822412896\n",
      "hidden layer  1  mean:  -0.0005549579144104043  stds:  0.9816538304245632\n",
      "hidden layer  2  mean:  0.0014949039550711666  stds:  0.9816399422945947\n",
      "hidden layer  3  mean:  -0.0008817035205276654  stds:  0.9816834677825703\n",
      "hidden layer  4  mean:  0.0018850459018778733  stds:  0.9817048813174646\n",
      "hidden layer  5  mean:  -0.0003096330152620919  stds:  0.9816054147332214\n",
      "hidden layer  6  mean:  -6.315937039574092e-05  stds:  0.9816712533745183\n",
      "hidden layer  7  mean:  0.0003860462823029906  stds:  0.9817401723118251\n",
      "hidden layer  8  mean:  -0.0018710890373545431  stds:  0.9816134053600307\n",
      "hidden layer  9  mean:  0.00015130619632828713  stds:  0.9816895275368257\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "#trying with different activations\n",
    "#declaring the input with size 500 and 1000 samples\n",
    "D = np.random.randn(1000,500)\n",
    "\n",
    "#creating 10 layers with 500 neurons each\n",
    "hidden_layers = [500]*10\n",
    "Hs = {}\n",
    "act = {\"relu\": lambda x: np.maximum(0,x), \"tanh\": lambda x: np.tanh(x), \"sigm\": lambda x: 1/(1+np.exp(-x))}\n",
    "nonlinearity = [\"tanh\"]*len(hidden_layers)\n",
    "for i in range(len(hidden_layers)):\n",
    "    X = D if i == 0 else Hs[i-1]\n",
    "    Win = X.shape[1]\n",
    "    Wout = hidden_layers[i]\n",
    "    \n",
    "    #xavier initialization\n",
    "    #W = np.random.randn(Win, Wout)/np.sqrt(Win)\n",
    "    W = 1*np.random.randn(Win, Wout)\n",
    "    \n",
    "    H = np.dot(X,W)\n",
    "    H = act[nonlinearity[i]](H)\n",
    "    Hs[i] = H\n",
    "    \n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "layers_mean = [np.mean(H) for i,H in Hs.items()]\n",
    "layers_stds = [np.std(H) for i,H in Hs.items()]\n",
    "\n",
    "for i in Hs:\n",
    "    print(\"hidden layer \", i, \" mean: \",layers_mean[i], \" stds: \", layers_stds[i])\n",
    "    \n",
    "plt.figure()\n",
    "plt.subplot(111)\n",
    "plt.plot(Hs.keys(), layers_mean, 'ob-')\n",
    "plt.title(\"layer mean\")\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(111)\n",
    "plt.plot(Hs.keys(), layers_stds, 'or-')\n",
    "plt.title(\"layer std\")\n",
    "    \n",
    "plt.figure()\n",
    "\n",
    "for i in range(len(Hs)):\n",
    "    plt.subplot(1, len(Hs), i+1)\n",
    "    plt.hist(Hs[i].ravel(), 30, range=(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierTrainer(object):\n",
    "  \"\"\" The trainer class performs SGD with momentum on a cost function \"\"\"\n",
    "  def __init__(self):\n",
    "    self.step_cache = {} # for storing velocities in momentum update\n",
    "\n",
    "  def train(self, X, y, X_val, y_val, \n",
    "            model, loss_function, \n",
    "            reg=0.0,\n",
    "            learning_rate=1e-2, momentum=0, learning_rate_decay=0.95,\n",
    "            update='momentum', sample_batches=True,\n",
    "            num_epochs=30, batch_size=100, acc_frequency=None,\n",
    "            verbose=False):\n",
    "    \"\"\"\n",
    "    Optimize the parameters of a model to minimize a loss function. We use\n",
    "    training data X and y to compute the loss and gradients, and periodically\n",
    "    check the accuracy on the validation set.\n",
    "    Inputs:\n",
    "    - X: Array of training data; each X[i] is a training sample.\n",
    "    - y: Vector of training labels; y[i] gives the label for X[i].\n",
    "    - X_val: Array of validation data\n",
    "    - y_val: Vector of validation labels\n",
    "    - model: Dictionary that maps parameter names to parameter values. Each\n",
    "      parameter value is a numpy array.\n",
    "    - loss_function: A function that can be called in the following ways:\n",
    "      scores = loss_function(X, model, reg=reg)\n",
    "      loss, grads = loss_function(X, model, y, reg=reg)\n",
    "    - reg: Regularization strength. This will be passed to the loss function.\n",
    "    - learning_rate: Initial learning rate to use.\n",
    "    - momentum: Parameter to use for momentum updates.\n",
    "    - learning_rate_decay: The learning rate is multiplied by this after each\n",
    "      epoch.\n",
    "    - update: The update rule to use. One of 'sgd', 'momentum', or 'rmsprop'.\n",
    "    - sample_batches: If True, use a minibatch of data for each parameter update\n",
    "      (stochastic gradient descent); if False, use the entire training set for\n",
    "      each parameter update (gradient descent).\n",
    "    - num_epochs: The number of epochs to take over the training data.\n",
    "    - batch_size: The number of training samples to use at each iteration.\n",
    "    - acc_frequency: If set to an integer, we compute the training and\n",
    "      validation set error after every acc_frequency iterations.\n",
    "    - verbose: If True, print status after each epoch.\n",
    "    Returns a tuple of:\n",
    "    - best_model: The model that got the highest validation accuracy during\n",
    "      training.\n",
    "    - loss_history: List containing the value of the loss function at each\n",
    "      iteration.\n",
    "    - train_acc_history: List storing the training set accuracy at each epoch.\n",
    "    - val_acc_history: List storing the validation set accuracy at each epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    N = X.shape[0]\n",
    "\n",
    "    if sample_batches:\n",
    "      iterations_per_epoch = N / batch_size # using SGD\n",
    "    else:\n",
    "      iterations_per_epoch = 1 # using GD\n",
    "    num_iters = num_epochs * iterations_per_epoch\n",
    "    epoch = 0\n",
    "    best_val_acc = 0.0\n",
    "    best_model = {}\n",
    "    loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "    for it in xrange(num_iters):\n",
    "      if it % 10 == 0:  print ('starting iteration ', it)\n",
    "\n",
    "      # get batch of data\n",
    "      if sample_batches:\n",
    "        batch_mask = np.random.choice(N, batch_size)\n",
    "        X_batch = X[batch_mask]\n",
    "        y_batch = y[batch_mask]\n",
    "      else:\n",
    "        # no SGD used, full gradient descent\n",
    "        X_batch = X\n",
    "        y_batch = y\n",
    "\n",
    "      # evaluate cost and gradient\n",
    "      cost, grads = loss_function(X_batch, model, y_batch, reg)\n",
    "      loss_history.append(cost)\n",
    "\n",
    "      # perform a parameter update\n",
    "      for p in model:\n",
    "        # compute the parameter step\n",
    "        if update == 'sgd':\n",
    "          dx = -learning_rate * grads[p]\n",
    "        elif update == 'momentum':\n",
    "          if not p in self.step_cache: \n",
    "            self.step_cache[p] = np.zeros(grads[p].shape)\n",
    "          #####################################################################\n",
    "          # TODO: implement the momentum update formula and store the step    #\n",
    "          # update into variable dx. You should use the variable              #\n",
    "          # step_cache[p] and the momentum strength is stored in momentum.    #\n",
    "          # Don't forget to also update the step_cache[p].                    #\n",
    "          #####################################################################\n",
    "          # Momentum update\n",
    "          dx = momentum * self.step_cache[p] - learning_rate * grads[p] # integrate velocity\n",
    "          self.step_cache[p] = dx\n",
    "          #####################################################################\n",
    "          #                      END OF YOUR CODE                             #\n",
    "          #####################################################################\n",
    "        elif update == 'rmsprop':\n",
    "          decay_rate = 0.99 # you could also make this an option\n",
    "          if not p in self.step_cache: \n",
    "            self.step_cache[p] = np.zeros(grads[p].shape)\n",
    "          #####################################################################\n",
    "          # TODO: implement the RMSProp update and store the parameter update #\n",
    "          # dx. Don't forget to also update step_cache[p]. Use smoothing 1e-8 #\n",
    "          #####################################################################\n",
    "          self.step_cache[p] = decay_rate * self.step_cache[p] + (1 - decay_rate) * grads[p]**2\n",
    "          dx = -learning_rate * grads[p] / np.sqrt(self.step_cache[p] + 1e-8)\n",
    "          #####################################################################\n",
    "          #                      END OF YOUR CODE                             #\n",
    "          #####################################################################\n",
    "        else:\n",
    "          raise ValueError('Unrecognized update type \"%s\"' % update)\n",
    "\n",
    "        # update the parameters\n",
    "        model[p] += dx\n",
    "\n",
    "      # every epoch perform an evaluation on the validation set\n",
    "      first_it = (it == 0)\n",
    "      epoch_end = (it + 1) % iterations_per_epoch == 0\n",
    "      acc_check = (acc_frequency is not None and it % acc_frequency == 0)\n",
    "      if first_it or epoch_end or acc_check:\n",
    "        if it > 0 and epoch_end:\n",
    "          # decay the learning rate\n",
    "          learning_rate *= learning_rate_decay\n",
    "          epoch += 1\n",
    "\n",
    "        # evaluate train accuracy\n",
    "        if N > 1000:\n",
    "          train_mask = np.random.choice(N, 1000)\n",
    "          X_train_subset = X[train_mask]\n",
    "          y_train_subset = y[train_mask]\n",
    "        else:\n",
    "          X_train_subset = X\n",
    "          y_train_subset = y\n",
    "        scores_train = loss_function(X_train_subset, model)\n",
    "        y_pred_train = np.argmax(scores_train, axis=1)\n",
    "        train_acc = np.mean(y_pred_train == y_train_subset)\n",
    "        train_acc_history.append(train_acc)\n",
    "\n",
    "        # evaluate val accuracy\n",
    "        scores_val = loss_function(X_val, model)\n",
    "        y_pred_val = np.argmax(scores_val, axis=1)\n",
    "        val_acc = np.mean(y_pred_val ==  y_val)\n",
    "        val_acc_history.append(val_acc)\n",
    "        \n",
    "        # keep track of the best model based on validation accuracy\n",
    "        if val_acc > best_val_acc:\n",
    "          # make a copy of the model\n",
    "          best_val_acc = val_acc\n",
    "          best_model = {}\n",
    "          for p in model:\n",
    "            best_model[p] = model[p].copy()\n",
    "\n",
    "        # print progress if needed\n",
    "        if verbose:\n",
    "          print ('Finished epoch %d / %d: cost %f, train: %f, val %f, lr %e'\n",
    "                 % (epoch, num_epochs, cost, train_acc, val_acc, learning_rate))\n",
    "\n",
    "    if verbose:\n",
    "      print ('finished optimization. best validation accuracy: %f' % (best_val_acc, ))\n",
    "    # return the best model and the training history statistics\n",
    "    return best_model, loss_history, train_acc_history, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_layer_net(X, model, y=None, reg=0.0):\n",
    "  \"\"\"\n",
    "  Compute the loss and gradients for a two layer fully connected neural network.\n",
    "  The net has an input dimension of D, a hidden layer dimension of H, and\n",
    "  performs classification over C classes. We use a softmax loss function and L2\n",
    "  regularization the the weight matrices. The two layer net should use a ReLU\n",
    "  nonlinearity after the first affine layer.\n",
    "  The two layer net has the following architecture:\n",
    "  input - fully connected layer - ReLU - fully connected layer - softmax\n",
    "  The outputs of the second fully-connected layer are the scores for each\n",
    "  class.\n",
    "  Inputs:\n",
    "  - X: Input data of shape (N, D). Each X[i] is a training sample.\n",
    "  - model: Dictionary mapping parameter names to arrays of parameter values.\n",
    "    It should contain the following:\n",
    "    - W1: First layer weights; has shape (D, H)\n",
    "    - b1: First layer biases; has shape (H,)\n",
    "    - W2: Second layer weights; has shape (H, C)\n",
    "    - b2: Second layer biases; has shape (C,)\n",
    "  - y: Vector of training labels. y[i] is the label for X[i], and each y[i] is\n",
    "    an integer in the range 0 <= y[i] < C. This parameter is optional; if it\n",
    "    is not passed then we only return scores, and if it is passed then we\n",
    "    instead return the loss and gradients.\n",
    "  - reg: Regularization strength.\n",
    "  Returns:\n",
    "  If y not is passed, return a matrix scores of shape (N, C) where scores[i, c]\n",
    "  is the score for class c on input X[i].\n",
    "  If y is not passed, instead return a tuple of:\n",
    "  - loss: Loss (data loss and regularization loss) for this batch of training\n",
    "    samples.\n",
    "  - grads: Dictionary mapping parameter names to gradients of those parameters\n",
    "    with respect to the loss function. This should have the same keys as model.\n",
    "  \"\"\"\n",
    "\n",
    "  # unpack variables from the model dictionary\n",
    "  W1,b1,W2,b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "  N, D = X.shape\n",
    "\n",
    "  # compute the forward pass\n",
    "  scores = None\n",
    "  #############################################################################\n",
    "  # TODO: Perform the forward pass, computing the class scores for the input. #\n",
    "  # Store the result in the scores variable, which should be an array of      #\n",
    "  # shape (N, C).                                                             #\n",
    "  #############################################################################\n",
    "  hidden_layer = np.maximum(0, np.dot(X, W1) + b1) # ReLU activation\n",
    "  scores = np.dot(hidden_layer, W2) + b2\n",
    "  #############################################################################\n",
    "  #                              END OF YOUR CODE                             #\n",
    "  #############################################################################\n",
    "  \n",
    "  # If the targets are not given then jump out, we're done\n",
    "  if y is None:\n",
    "    return scores\n",
    "\n",
    "  # compute the loss\n",
    "  loss = None\n",
    "  #############################################################################\n",
    "  # TODO: Finish the forward pass, and compute the loss. This should include  #\n",
    "  # both the data loss and L2 regularization for W1 and W2. Store the result  #\n",
    "  # in the variable loss, which should be a scalar. Use the Softmax           #\n",
    "  # classifier loss. So that your results match ours, multiply the            #\n",
    "  # regularization loss by 0.5                                                #\n",
    "  #############################################################################\n",
    "  # compute the class probabilities\n",
    "  exp_scores = np.exp(scores)\n",
    "  probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "\n",
    "  # average cross-entropy loss and regularization\n",
    "  corect_logprobs = -np.log(probs[range(N),y])\n",
    "  data_loss = np.sum(corect_logprobs)/N\n",
    "  reg_loss = 0.5*reg*np.sum(W1*W1) + 0.5*reg*np.sum(W2*W2)\n",
    "  loss = data_loss + reg_loss\n",
    "  #############################################################################\n",
    "  #                              END OF YOUR CODE                             #\n",
    "  #############################################################################\n",
    "\n",
    "  # compute the gradients\n",
    "  grads = {}\n",
    "  #############################################################################\n",
    "  # TODO: Compute the backward pass, computing the derivatives of the weights #\n",
    "  # and biases. Store the results in the grads dictionary. For example,       #\n",
    "  # grads['W1'] should store the gradient on W1, and be a matrix of same size #\n",
    "  #############################################################################\n",
    "  # compute the gradient on scores\n",
    "  dscores = probs\n",
    "  dscores[range(N),y] -= 1\n",
    "  dscores /= N\n",
    "\n",
    "  # W2 and b2\n",
    "  grads['W2'] = np.dot(hidden_layer.T, dscores)\n",
    "  grads['b2'] = np.sum(dscores, axis=0)\n",
    "  # next backprop into hidden layer\n",
    "  dhidden = np.dot(dscores, W2.T)\n",
    "  # backprop the ReLU non-linearity\n",
    "  dhidden[hidden_layer <= 0] = 0\n",
    "  # finally into W,b\n",
    "  grads['W1'] = np.dot(X.T, dhidden)\n",
    "  grads['b1'] = np.sum(dhidden, axis=0)\n",
    "\n",
    "  # add regularization gradient contribution\n",
    "  grads['W2'] += reg * W2\n",
    "  grads['W1'] += reg * W1\n",
    "  #############################################################################\n",
    "  #                              END OF YOUR CODE                             #\n",
    "  #############################################################################\n",
    "\n",
    "  return loss, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 59  43  50 ... 140  84  72]\n",
      " [154 126 105 ... 139 142 144]\n",
      " [255 253 253 ...  83  83  84]\n",
      " ...\n",
      " [ 45  42  35 ...  53  29  31]\n",
      " [128 121 138 ... 145 142 146]\n",
      " [202 202 204 ... 243 243 243]] [6 9 9 4 1 1 2 7 8 3 4 7 7 2 9 9 9 3 2 6 4 3 6 6 2 6 3 5 4 0]\n",
      "starting iteration  0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (100,3072) and (5,5) not aligned: 3072 (dim 1) != 5 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-186-468a4ccd6f54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'momentum'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             verbose=False)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-184-7820044122a9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y, X_val, y_val, model, loss_function, reg, learning_rate, momentum, learning_rate_decay, update, sample_batches, num_epochs, batch_size, acc_frequency, verbose)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0;31m# evaluate cost and gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m       \u001b[0mloss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-185-636cc7c0c6f0>\u001b[0m in \u001b[0;36mtwo_layer_net\u001b[0;34m(X, model, y, reg)\u001b[0m\n\u001b[1;32m     44\u001b[0m   \u001b[0;31m# shape (N, C).                                                             #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m   \u001b[0;31m#############################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m   \u001b[0mhidden_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# ReLU activation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m   \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0;31m#############################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (100,3072) and (5,5) not aligned: 3072 (dim 1) != 5 (dim 0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "model = {}\n",
    "\n",
    "hs = [5,5,5,5,5]\n",
    "for i in range(len(hs)-1):\n",
    "    sz_in = hs[i]\n",
    "    sz_out = hs[i+1]\n",
    "    wstr = 'W'+str(i+1)\n",
    "    bstr = 'b'+str(i+1)\n",
    "    model[wstr] = np.random.randn(sz_in, sz_out)\n",
    "    model[bstr] = np.zeros(sz_out)\n",
    "    \n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "trainer = ClassifierTrainer()\n",
    "dic = unpickle(\"/Users/anirudh/Desktop/cifar-10-batches-py/data_batch_1\")\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for i in range(30):\n",
    "    X.append(dic[b'data'][i])\n",
    "    y.append(dic[b'labels'][i])\n",
    "    \n",
    "X = np.asarray(X)\n",
    "y = np.asarray(y)\n",
    "\n",
    "print(X, y)\n",
    "trainer.train(X, y, X, y, \n",
    "            model, two_layer_net, \n",
    "            reg=0.0,\n",
    "            learning_rate=1e-2, momentum=0, learning_rate_decay=0.95,\n",
    "            update='momentum', sample_batches=True,\n",
    "            num_epochs=200,\n",
    "            verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.69314718, -1.38629436, -1.38629436, -2.19722458])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([[1,2],[3,4],[45,4],[13,9]])\n",
    "k = -np.log(x[range(4),1])\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 3.1 % 1 == 0:\n",
    "    print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0001', '01', '0001', '01', '1'], ['0001', '01', '000101', '1'], ['000101', '0001', '01', '1'], ['000101', '000101', '1']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#In how many different way a binary string can have numbers that are powers of 5\n",
    "#DFS ITERATIONS LOVING MY OWN SOLUTIONS :):):):) <3<3\n",
    "\n",
    "def power(out, x):\n",
    "    while out > 1:\n",
    "        out /= 5\n",
    "        \n",
    "    if out == 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def split_power_5(st, j, final, temp = []):\n",
    "\n",
    "    if j == len(st):\n",
    "        final.append(list(temp))\n",
    "        return\n",
    "    \n",
    "    sumc = \"\"\n",
    "    \n",
    "    for i in range(j, len(st)):\n",
    "        sumc += st[i]\n",
    "        #print(sumc)\n",
    "        if power(int(sumc, 2), 5):\n",
    "            temp.append(sumc)\n",
    "            split_power_5(st, i+1, final, temp)\n",
    "            temp.pop()\n",
    "            \n",
    "final = []\n",
    "st = \"0001010001011\"\n",
    "split_power_5(st, 0, final)\n",
    "\n",
    "print(final)\n",
    "\n",
    "power(1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'c', 's', 'a', 's', 'd', 's']\n",
      "['a', 'c', 's', 'a', 's', 'd', 's']\n",
      "['a', 'c', 's', 'd', 's', 'a', 's']\n",
      "['a', 'c', 's', 'd', 's', 'a', 's']\n",
      "['a', 'c', 's', 'a', 's', 'd', 's']\n",
      "['a', 'c', 's', 'a', 's', 'd', 's']\n",
      "['a', 'c', 's', 'd', 's', 'a', 's']\n",
      "['a', 'c', 's', 'd', 's', 'a', 's']\n",
      "['a', 'c', 's', 'a', 's', 'd', 's']\n",
      "['a', 'c', 's', 'a', 's', 'd', 's']\n",
      "['a', 'c', 's', 'd', 's', 'a', 's']\n",
      "['a', 'c', 's', 'd', 's', 'a', 's']\n",
      "['a', 'd', 's', 'a', 's', 'c', 's']\n",
      "['a', 'd', 's', 'a', 's', 'c', 's']\n",
      "['a', 'd', 's', 'c', 's', 'a', 's']\n",
      "['a', 'd', 's', 'c', 's', 'a', 's']\n",
      "['a', 'd', 's', 'a', 's', 'c', 's']\n",
      "['a', 'd', 's', 'a', 's', 'c', 's']\n",
      "['a', 'd', 's', 'c', 's', 'a', 's']\n",
      "['a', 'd', 's', 'c', 's', 'a', 's']\n",
      "['a', 'd', 's', 'a', 's', 'c', 's']\n",
      "['a', 'd', 's', 'a', 's', 'c', 's']\n",
      "['a', 'd', 's', 'c', 's', 'a', 's']\n",
      "['a', 'd', 's', 'c', 's', 'a', 's']\n",
      "['a', 's', 'a', 'c', 's', 'd', 's']\n",
      "['a', 's', 'a', 'c', 's', 'd', 's']\n",
      "['a', 's', 'a', 'd', 's', 'c', 's']\n",
      "['a', 's', 'a', 'd', 's', 'c', 's']\n",
      "['a', 's', 'a', 's', 'c', 'd', 's']\n",
      "['a', 's', 'a', 's', 'c', 's', 'd']\n",
      "['a', 's', 'a', 's', 'd', 'c', 's']\n",
      "['a', 's', 'a', 's', 'd', 's', 'c']\n",
      "['a', 's', 'a', 's', 'c', 'd', 's']\n",
      "['a', 's', 'a', 's', 'c', 's', 'd']\n",
      "['a', 's', 'a', 's', 'd', 'c', 's']\n",
      "['a', 's', 'a', 's', 'd', 's', 'c']\n",
      "['a', 's', 'c', 'a', 's', 'd', 's']\n",
      "['a', 's', 'c', 'a', 's', 'd', 's']\n",
      "['a', 's', 'c', 'd', 's', 'a', 's']\n",
      "['a', 's', 'c', 'd', 's', 'a', 's']\n",
      "['a', 's', 'c', 's', 'a', 'd', 's']\n",
      "['a', 's', 'c', 's', 'a', 's', 'd']\n",
      "['a', 's', 'c', 's', 'd', 'a', 's']\n",
      "['a', 's', 'c', 's', 'd', 's', 'a']\n",
      "['a', 's', 'c', 's', 'a', 'd', 's']\n",
      "['a', 's', 'c', 's', 'a', 's', 'd']\n",
      "['a', 's', 'c', 's', 'd', 'a', 's']\n",
      "['a', 's', 'c', 's', 'd', 's', 'a']\n",
      "['a', 's', 'd', 'a', 's', 'c', 's']\n",
      "['a', 's', 'd', 'a', 's', 'c', 's']\n",
      "['a', 's', 'd', 'c', 's', 'a', 's']\n",
      "['a', 's', 'd', 'c', 's', 'a', 's']\n",
      "['a', 's', 'd', 's', 'a', 'c', 's']\n",
      "['a', 's', 'd', 's', 'a', 's', 'c']\n",
      "['a', 's', 'd', 's', 'c', 'a', 's']\n",
      "['a', 's', 'd', 's', 'c', 's', 'a']\n",
      "['a', 's', 'd', 's', 'a', 'c', 's']\n",
      "['a', 's', 'd', 's', 'a', 's', 'c']\n",
      "['a', 's', 'd', 's', 'c', 'a', 's']\n",
      "['a', 's', 'd', 's', 'c', 's', 'a']\n",
      "['a', 's', 'a', 'c', 's', 'd', 's']\n",
      "['a', 's', 'a', 'c', 's', 'd', 's']\n",
      "['a', 's', 'a', 'd', 's', 'c', 's']\n",
      "['a', 's', 'a', 'd', 's', 'c', 's']\n",
      "['a', 's', 'a', 's', 'c', 'd', 's']\n",
      "['a', 's', 'a', 's', 'c', 's', 'd']\n",
      "['a', 's', 'a', 's', 'd', 'c', 's']\n",
      "['a', 's', 'a', 's', 'd', 's', 'c']\n",
      "['a', 's', 'a', 's', 'c', 'd', 's']\n",
      "['a', 's', 'a', 's', 'c', 's', 'd']\n",
      "['a', 's', 'a', 's', 'd', 'c', 's']\n",
      "['a', 's', 'a', 's', 'd', 's', 'c']\n",
      "['a', 's', 'c', 'a', 's', 'd', 's']\n",
      "['a', 's', 'c', 'a', 's', 'd', 's']\n",
      "['a', 's', 'c', 'd', 's', 'a', 's']\n",
      "['a', 's', 'c', 'd', 's', 'a', 's']\n",
      "['a', 's', 'c', 's', 'a', 'd', 's']\n",
      "['a', 's', 'c', 's', 'a', 's', 'd']\n",
      "['a', 's', 'c', 's', 'd', 'a', 's']\n",
      "['a', 's', 'c', 's', 'd', 's', 'a']\n",
      "['a', 's', 'c', 's', 'a', 'd', 's']\n",
      "['a', 's', 'c', 's', 'a', 's', 'd']\n",
      "['a', 's', 'c', 's', 'd', 'a', 's']\n",
      "['a', 's', 'c', 's', 'd', 's', 'a']\n",
      "['a', 's', 'd', 'a', 's', 'c', 's']\n",
      "['a', 's', 'd', 'a', 's', 'c', 's']\n",
      "['a', 's', 'd', 'c', 's', 'a', 's']\n",
      "['a', 's', 'd', 'c', 's', 'a', 's']\n",
      "['a', 's', 'd', 's', 'a', 'c', 's']\n",
      "['a', 's', 'd', 's', 'a', 's', 'c']\n",
      "['a', 's', 'd', 's', 'c', 'a', 's']\n",
      "['a', 's', 'd', 's', 'c', 's', 'a']\n",
      "['a', 's', 'd', 's', 'a', 'c', 's']\n",
      "['a', 's', 'd', 's', 'a', 's', 'c']\n",
      "['a', 's', 'd', 's', 'c', 'a', 's']\n",
      "['a', 's', 'd', 's', 'c', 's', 'a']\n",
      "['a', 's', 'a', 'c', 's', 'd', 's']\n",
      "['a', 's', 'a', 'c', 's', 'd', 's']\n",
      "['a', 's', 'a', 'd', 's', 'c', 's']\n",
      "['a', 's', 'a', 'd', 's', 'c', 's']\n",
      "['a', 's', 'a', 's', 'c', 'd', 's']\n",
      "['a', 's', 'a', 's', 'c', 's', 'd']\n",
      "['a', 's', 'a', 's', 'd', 'c', 's']\n",
      "['a', 's', 'a', 's', 'd', 's', 'c']\n",
      "['a', 's', 'a', 's', 'c', 'd', 's']\n",
      "['a', 's', 'a', 's', 'c', 's', 'd']\n",
      "['a', 's', 'a', 's', 'd', 'c', 's']\n",
      "['a', 's', 'a', 's', 'd', 's', 'c']\n",
      "['a', 's', 'c', 'a', 's', 'd', 's']\n",
      "['a', 's', 'c', 'a', 's', 'd', 's']\n",
      "['a', 's', 'c', 'd', 's', 'a', 's']\n",
      "['a', 's', 'c', 'd', 's', 'a', 's']\n",
      "['a', 's', 'c', 's', 'a', 'd', 's']\n",
      "['a', 's', 'c', 's', 'a', 's', 'd']\n",
      "['a', 's', 'c', 's', 'd', 'a', 's']\n",
      "['a', 's', 'c', 's', 'd', 's', 'a']\n",
      "['a', 's', 'c', 's', 'a', 'd', 's']\n",
      "['a', 's', 'c', 's', 'a', 's', 'd']\n",
      "['a', 's', 'c', 's', 'd', 'a', 's']\n",
      "['a', 's', 'c', 's', 'd', 's', 'a']\n",
      "['a', 's', 'd', 'a', 's', 'c', 's']\n",
      "['a', 's', 'd', 'a', 's', 'c', 's']\n",
      "['a', 's', 'd', 'c', 's', 'a', 's']\n",
      "['a', 's', 'd', 'c', 's', 'a', 's']\n",
      "['a', 's', 'd', 's', 'a', 'c', 's']\n",
      "['a', 's', 'd', 's', 'a', 's', 'c']\n",
      "['a', 's', 'd', 's', 'c', 'a', 's']\n",
      "['a', 's', 'd', 's', 'c', 's', 'a']\n",
      "['a', 's', 'd', 's', 'a', 'c', 's']\n",
      "['a', 's', 'd', 's', 'a', 's', 'c']\n",
      "['a', 's', 'd', 's', 'c', 'a', 's']\n",
      "['a', 's', 'd', 's', 'c', 's', 'a']\n",
      "['a', 'c', 's', 'a', 's', 'd', 's']\n",
      "['a', 'c', 's', 'a', 's', 'd', 's']\n",
      "['a', 'c', 's', 'd', 's', 'a', 's']\n",
      "['a', 'c', 's', 'd', 's', 'a', 's']\n",
      "['a', 'c', 's', 'a', 's', 'd', 's']\n",
      "['a', 'c', 's', 'a', 's', 'd', 's']\n",
      "['a', 'c', 's', 'd', 's', 'a', 's']\n",
      "['a', 'c', 's', 'd', 's', 'a', 's']\n",
      "['a', 'c', 's', 'a', 's', 'd', 's']\n",
      "['a', 'c', 's', 'a', 's', 'd', 's']\n",
      "['a', 'c', 's', 'd', 's', 'a', 's']\n",
      "['a', 'c', 's', 'd', 's', 'a', 's']\n",
      "['a', 'd', 's', 'a', 's', 'c', 's']\n",
      "['a', 'd', 's', 'a', 's', 'c', 's']\n",
      "['a', 'd', 's', 'c', 's', 'a', 's']\n",
      "['a', 'd', 's', 'c', 's', 'a', 's']\n",
      "['a', 'd', 's', 'a', 's', 'c', 's']\n",
      "['a', 'd', 's', 'a', 's', 'c', 's']\n",
      "['a', 'd', 's', 'c', 's', 'a', 's']\n",
      "['a', 'd', 's', 'c', 's', 'a', 's']\n",
      "['a', 'd', 's', 'a', 's', 'c', 's']\n",
      "['a', 'd', 's', 'a', 's', 'c', 's']\n",
      "['a', 'd', 's', 'c', 's', 'a', 's']\n",
      "['a', 'd', 's', 'c', 's', 'a', 's']\n",
      "['a', 's', 'a', 'c', 's', 'd', 's']\n",
      "['a', 's', 'a', 'c', 's', 'd', 's']\n",
      "['a', 's', 'a', 'd', 's', 'c', 's']\n",
      "['a', 's', 'a', 'd', 's', 'c', 's']\n",
      "['a', 's', 'a', 's', 'c', 'd', 's']\n",
      "['a', 's', 'a', 's', 'c', 's', 'd']\n",
      "['a', 's', 'a', 's', 'd', 'c', 's']\n",
      "['a', 's', 'a', 's', 'd', 's', 'c']\n",
      "['a', 's', 'a', 's', 'c', 'd', 's']\n",
      "['a', 's', 'a', 's', 'c', 's', 'd']\n",
      "['a', 's', 'a', 's', 'd', 'c', 's']\n",
      "['a', 's', 'a', 's', 'd', 's', 'c']\n",
      "['a', 's', 'c', 'a', 's', 'd', 's']\n",
      "['a', 's', 'c', 'a', 's', 'd', 's']\n",
      "['a', 's', 'c', 'd', 's', 'a', 's']\n",
      "['a', 's', 'c', 'd', 's', 'a', 's']\n",
      "['a', 's', 'c', 's', 'a', 'd', 's']\n",
      "['a', 's', 'c', 's', 'a', 's', 'd']\n",
      "['a', 's', 'c', 's', 'd', 'a', 's']\n",
      "['a', 's', 'c', 's', 'd', 's', 'a']\n",
      "['a', 's', 'c', 's', 'a', 'd', 's']\n",
      "['a', 's', 'c', 's', 'a', 's', 'd']\n",
      "['a', 's', 'c', 's', 'd', 'a', 's']\n",
      "['a', 's', 'c', 's', 'd', 's', 'a']\n",
      "['a', 's', 'd', 'a', 's', 'c', 's']\n",
      "['a', 's', 'd', 'a', 's', 'c', 's']\n",
      "['a', 's', 'd', 'c', 's', 'a', 's']\n",
      "['a', 's', 'd', 'c', 's', 'a', 's']\n",
      "['a', 's', 'd', 's', 'a', 'c', 's']\n",
      "['a', 's', 'd', 's', 'a', 's', 'c']\n",
      "['a', 's', 'd', 's', 'c', 'a', 's']\n",
      "['a', 's', 'd', 's', 'c', 's', 'a']\n",
      "['a', 's', 'd', 's', 'a', 'c', 's']\n",
      "['a', 's', 'd', 's', 'a', 's', 'c']\n",
      "['a', 's', 'd', 's', 'c', 'a', 's']\n",
      "['a', 's', 'd', 's', 'c', 's', 'a']\n",
      "['a', 's', 'a', 'c', 's', 'd', 's']\n",
      "['a', 's', 'a', 'c', 's', 'd', 's']\n",
      "['a', 's', 'a', 'd', 's', 'c', 's']\n",
      "['a', 's', 'a', 'd', 's', 'c', 's']\n",
      "['a', 's', 'a', 's', 'c', 'd', 's']\n",
      "['a', 's', 'a', 's', 'c', 's', 'd']\n",
      "['a', 's', 'a', 's', 'd', 'c', 's']\n",
      "['a', 's', 'a', 's', 'd', 's', 'c']\n",
      "['a', 's', 'a', 's', 'c', 'd', 's']\n",
      "['a', 's', 'a', 's', 'c', 's', 'd']\n",
      "['a', 's', 'a', 's', 'd', 'c', 's']\n",
      "['a', 's', 'a', 's', 'd', 's', 'c']\n",
      "['a', 's', 'c', 'a', 's', 'd', 's']\n",
      "['a', 's', 'c', 'a', 's', 'd', 's']\n",
      "['a', 's', 'c', 'd', 's', 'a', 's']\n",
      "['a', 's', 'c', 'd', 's', 'a', 's']\n",
      "['a', 's', 'c', 's', 'a', 'd', 's']\n",
      "['a', 's', 'c', 's', 'a', 's', 'd']\n",
      "['a', 's', 'c', 's', 'd', 'a', 's']\n",
      "['a', 's', 'c', 's', 'd', 's', 'a']\n",
      "['a', 's', 'c', 's', 'a', 'd', 's']\n",
      "['a', 's', 'c', 's', 'a', 's', 'd']\n",
      "['a', 's', 'c', 's', 'd', 'a', 's']\n",
      "['a', 's', 'c', 's', 'd', 's', 'a']\n",
      "['a', 's', 'd', 'a', 's', 'c', 's']\n",
      "['a', 's', 'd', 'a', 's', 'c', 's']\n",
      "['a', 's', 'd', 'c', 's', 'a', 's']\n",
      "['a', 's', 'd', 'c', 's', 'a', 's']\n",
      "['a', 's', 'd', 's', 'a', 'c', 's']\n",
      "['a', 's', 'd', 's', 'a', 's', 'c']\n",
      "['a', 's', 'd', 's', 'c', 'a', 's']\n",
      "['a', 's', 'd', 's', 'c', 's', 'a']\n",
      "['a', 's', 'd', 's', 'a', 'c', 's']\n",
      "['a', 's', 'd', 's', 'a', 's', 'c']\n",
      "['a', 's', 'd', 's', 'c', 'a', 's']\n",
      "['a', 's', 'd', 's', 'c', 's', 'a']\n",
      "['a', 's', 'a', 'c', 's', 'd', 's']\n",
      "['a', 's', 'a', 'c', 's', 'd', 's']\n",
      "['a', 's', 'a', 'd', 's', 'c', 's']\n",
      "['a', 's', 'a', 'd', 's', 'c', 's']\n",
      "['a', 's', 'a', 's', 'c', 'd', 's']\n",
      "['a', 's', 'a', 's', 'c', 's', 'd']\n",
      "['a', 's', 'a', 's', 'd', 'c', 's']\n",
      "['a', 's', 'a', 's', 'd', 's', 'c']\n",
      "['a', 's', 'a', 's', 'c', 'd', 's']\n",
      "['a', 's', 'a', 's', 'c', 's', 'd']\n",
      "['a', 's', 'a', 's', 'd', 'c', 's']\n",
      "['a', 's', 'a', 's', 'd', 's', 'c']\n",
      "['a', 's', 'c', 'a', 's', 'd', 's']\n",
      "['a', 's', 'c', 'a', 's', 'd', 's']\n",
      "['a', 's', 'c', 'd', 's', 'a', 's']\n",
      "['a', 's', 'c', 'd', 's', 'a', 's']\n",
      "['a', 's', 'c', 's', 'a', 'd', 's']\n",
      "['a', 's', 'c', 's', 'a', 's', 'd']\n",
      "['a', 's', 'c', 's', 'd', 'a', 's']\n",
      "['a', 's', 'c', 's', 'd', 's', 'a']\n",
      "['a', 's', 'c', 's', 'a', 'd', 's']\n",
      "['a', 's', 'c', 's', 'a', 's', 'd']\n",
      "['a', 's', 'c', 's', 'd', 'a', 's']\n",
      "['a', 's', 'c', 's', 'd', 's', 'a']\n",
      "['a', 's', 'd', 'a', 's', 'c', 's']\n",
      "['a', 's', 'd', 'a', 's', 'c', 's']\n",
      "['a', 's', 'd', 'c', 's', 'a', 's']\n",
      "['a', 's', 'd', 'c', 's', 'a', 's']\n",
      "['a', 's', 'd', 's', 'a', 'c', 's']\n",
      "['a', 's', 'd', 's', 'a', 's', 'c']\n",
      "['a', 's', 'd', 's', 'c', 'a', 's']\n",
      "['a', 's', 'd', 's', 'c', 's', 'a']\n",
      "['a', 's', 'd', 's', 'a', 'c', 's']\n",
      "['a', 's', 'd', 's', 'a', 's', 'c']\n",
      "['a', 's', 'd', 's', 'c', 'a', 's']\n",
      "['a', 's', 'd', 's', 'c', 's', 'a']\n",
      "['c', 'a', 's', 'a', 's', 'd', 's']\n",
      "['c', 'a', 's', 'a', 's', 'd', 's']\n",
      "['c', 'a', 's', 'd', 's', 'a', 's']\n",
      "['c', 'a', 's', 'd', 's', 'a', 's']\n",
      "['c', 'a', 's', 'a', 's', 'd', 's']\n",
      "['c', 'a', 's', 'a', 's', 'd', 's']\n",
      "['c', 'a', 's', 'd', 's', 'a', 's']\n",
      "['c', 'a', 's', 'd', 's', 'a', 's']\n",
      "['c', 'a', 's', 'a', 's', 'd', 's']\n",
      "['c', 'a', 's', 'a', 's', 'd', 's']\n",
      "['c', 'a', 's', 'd', 's', 'a', 's']\n",
      "['c', 'a', 's', 'd', 's', 'a', 's']\n",
      "['c', 'a', 's', 'a', 's', 'd', 's']\n",
      "['c', 'a', 's', 'a', 's', 'd', 's']\n",
      "['c', 'a', 's', 'd', 's', 'a', 's']\n",
      "['c', 'a', 's', 'd', 's', 'a', 's']\n",
      "['c', 'a', 's', 'a', 's', 'd', 's']\n",
      "['c', 'a', 's', 'a', 's', 'd', 's']\n",
      "['c', 'a', 's', 'd', 's', 'a', 's']\n",
      "['c', 'a', 's', 'd', 's', 'a', 's']\n",
      "['c', 'a', 's', 'a', 's', 'd', 's']\n",
      "['c', 'a', 's', 'a', 's', 'd', 's']\n",
      "['c', 'a', 's', 'd', 's', 'a', 's']\n",
      "['c', 'a', 's', 'd', 's', 'a', 's']\n",
      "['c', 'd', 's', 'a', 's', 'a', 's']\n",
      "['c', 'd', 's', 'a', 's', 'a', 's']\n",
      "['c', 'd', 's', 'a', 's', 'a', 's']\n",
      "['c', 'd', 's', 'a', 's', 'a', 's']\n",
      "['c', 'd', 's', 'a', 's', 'a', 's']\n",
      "['c', 'd', 's', 'a', 's', 'a', 's']\n",
      "['c', 'd', 's', 'a', 's', 'a', 's']\n",
      "['c', 'd', 's', 'a', 's', 'a', 's']\n",
      "['c', 'd', 's', 'a', 's', 'a', 's']\n",
      "['c', 'd', 's', 'a', 's', 'a', 's']\n",
      "['c', 'd', 's', 'a', 's', 'a', 's']\n",
      "['c', 'd', 's', 'a', 's', 'a', 's']\n",
      "['c', 's', 'a', 'd', 's', 'a', 's']\n",
      "['c', 's', 'a', 'd', 's', 'a', 's']\n",
      "['c', 's', 'a', 's', 'a', 'd', 's']\n",
      "['c', 's', 'a', 's', 'a', 's', 'd']\n",
      "['c', 's', 'a', 's', 'd', 'a', 's']\n",
      "['c', 's', 'a', 's', 'd', 's', 'a']\n",
      "['c', 's', 'a', 's', 'a', 'd', 's']\n",
      "['c', 's', 'a', 's', 'a', 's', 'd']\n",
      "['c', 's', 'a', 's', 'd', 'a', 's']\n",
      "['c', 's', 'a', 's', 'd', 's', 'a']\n",
      "['c', 's', 'a', 'd', 's', 'a', 's']\n",
      "['c', 's', 'a', 'd', 's', 'a', 's']\n",
      "['c', 's', 'a', 's', 'a', 'd', 's']\n",
      "['c', 's', 'a', 's', 'a', 's', 'd']\n",
      "['c', 's', 'a', 's', 'd', 'a', 's']\n",
      "['c', 's', 'a', 's', 'd', 's', 'a']\n",
      "['c', 's', 'a', 's', 'a', 'd', 's']\n",
      "['c', 's', 'a', 's', 'a', 's', 'd']\n",
      "['c', 's', 'a', 's', 'd', 'a', 's']\n",
      "['c', 's', 'a', 's', 'd', 's', 'a']\n",
      "['c', 's', 'd', 'a', 's', 'a', 's']\n",
      "['c', 's', 'd', 'a', 's', 'a', 's']\n",
      "['c', 's', 'd', 'a', 's', 'a', 's']\n",
      "['c', 's', 'd', 'a', 's', 'a', 's']\n",
      "['c', 's', 'd', 's', 'a', 's', 'a']\n",
      "['c', 's', 'd', 's', 'a', 's', 'a']\n",
      "['c', 's', 'd', 's', 'a', 's', 'a']\n",
      "['c', 's', 'd', 's', 'a', 's', 'a']\n",
      "['c', 's', 'a', 'd', 's', 'a', 's']\n",
      "['c', 's', 'a', 'd', 's', 'a', 's']\n",
      "['c', 's', 'a', 's', 'a', 'd', 's']\n",
      "['c', 's', 'a', 's', 'a', 's', 'd']\n",
      "['c', 's', 'a', 's', 'd', 'a', 's']\n",
      "['c', 's', 'a', 's', 'd', 's', 'a']\n",
      "['c', 's', 'a', 's', 'a', 'd', 's']\n",
      "['c', 's', 'a', 's', 'a', 's', 'd']\n",
      "['c', 's', 'a', 's', 'd', 'a', 's']\n",
      "['c', 's', 'a', 's', 'd', 's', 'a']\n",
      "['c', 's', 'a', 'd', 's', 'a', 's']\n",
      "['c', 's', 'a', 'd', 's', 'a', 's']\n",
      "['c', 's', 'a', 's', 'a', 'd', 's']\n",
      "['c', 's', 'a', 's', 'a', 's', 'd']\n",
      "['c', 's', 'a', 's', 'd', 'a', 's']\n",
      "['c', 's', 'a', 's', 'd', 's', 'a']\n",
      "['c', 's', 'a', 's', 'a', 'd', 's']\n",
      "['c', 's', 'a', 's', 'a', 's', 'd']\n",
      "['c', 's', 'a', 's', 'd', 'a', 's']\n",
      "['c', 's', 'a', 's', 'd', 's', 'a']\n",
      "['c', 's', 'd', 'a', 's', 'a', 's']\n",
      "['c', 's', 'd', 'a', 's', 'a', 's']\n",
      "['c', 's', 'd', 'a', 's', 'a', 's']\n",
      "['c', 's', 'd', 'a', 's', 'a', 's']\n",
      "['c', 's', 'd', 's', 'a', 's', 'a']\n",
      "['c', 's', 'd', 's', 'a', 's', 'a']\n",
      "['c', 's', 'd', 's', 'a', 's', 'a']\n",
      "['c', 's', 'd', 's', 'a', 's', 'a']\n",
      "['c', 's', 'a', 'd', 's', 'a', 's']\n",
      "['c', 's', 'a', 'd', 's', 'a', 's']\n",
      "['c', 's', 'a', 's', 'a', 'd', 's']\n",
      "['c', 's', 'a', 's', 'a', 's', 'd']\n",
      "['c', 's', 'a', 's', 'd', 'a', 's']\n",
      "['c', 's', 'a', 's', 'd', 's', 'a']\n",
      "['c', 's', 'a', 's', 'a', 'd', 's']\n",
      "['c', 's', 'a', 's', 'a', 's', 'd']\n",
      "['c', 's', 'a', 's', 'd', 'a', 's']\n",
      "['c', 's', 'a', 's', 'd', 's', 'a']\n",
      "['c', 's', 'a', 'd', 's', 'a', 's']\n",
      "['c', 's', 'a', 'd', 's', 'a', 's']\n",
      "['c', 's', 'a', 's', 'a', 'd', 's']\n",
      "['c', 's', 'a', 's', 'a', 's', 'd']\n",
      "['c', 's', 'a', 's', 'd', 'a', 's']\n",
      "['c', 's', 'a', 's', 'd', 's', 'a']\n",
      "['c', 's', 'a', 's', 'a', 'd', 's']\n",
      "['c', 's', 'a', 's', 'a', 's', 'd']\n",
      "['c', 's', 'a', 's', 'd', 'a', 's']\n",
      "['c', 's', 'a', 's', 'd', 's', 'a']\n",
      "['c', 's', 'd', 'a', 's', 'a', 's']\n",
      "['c', 's', 'd', 'a', 's', 'a', 's']\n",
      "['c', 's', 'd', 'a', 's', 'a', 's']\n",
      "['c', 's', 'd', 'a', 's', 'a', 's']\n",
      "['c', 's', 'd', 's', 'a', 's', 'a']\n",
      "['c', 's', 'd', 's', 'a', 's', 'a']\n",
      "['c', 's', 'd', 's', 'a', 's', 'a']\n",
      "['c', 's', 'd', 's', 'a', 's', 'a']\n",
      "['d', 'a', 's', 'a', 's', 'c', 's']\n",
      "['d', 'a', 's', 'a', 's', 'c', 's']\n",
      "['d', 'a', 's', 'c', 's', 'a', 's']\n",
      "['d', 'a', 's', 'c', 's', 'a', 's']\n",
      "['d', 'a', 's', 'a', 's', 'c', 's']\n",
      "['d', 'a', 's', 'a', 's', 'c', 's']\n",
      "['d', 'a', 's', 'c', 's', 'a', 's']\n",
      "['d', 'a', 's', 'c', 's', 'a', 's']\n",
      "['d', 'a', 's', 'a', 's', 'c', 's']\n",
      "['d', 'a', 's', 'a', 's', 'c', 's']\n",
      "['d', 'a', 's', 'c', 's', 'a', 's']\n",
      "['d', 'a', 's', 'c', 's', 'a', 's']\n",
      "['d', 'a', 's', 'a', 's', 'c', 's']\n",
      "['d', 'a', 's', 'a', 's', 'c', 's']\n",
      "['d', 'a', 's', 'c', 's', 'a', 's']\n",
      "['d', 'a', 's', 'c', 's', 'a', 's']\n",
      "['d', 'a', 's', 'a', 's', 'c', 's']\n",
      "['d', 'a', 's', 'a', 's', 'c', 's']\n",
      "['d', 'a', 's', 'c', 's', 'a', 's']\n",
      "['d', 'a', 's', 'c', 's', 'a', 's']\n",
      "['d', 'a', 's', 'a', 's', 'c', 's']\n",
      "['d', 'a', 's', 'a', 's', 'c', 's']\n",
      "['d', 'a', 's', 'c', 's', 'a', 's']\n",
      "['d', 'a', 's', 'c', 's', 'a', 's']\n",
      "['d', 'c', 's', 'a', 's', 'a', 's']\n",
      "['d', 'c', 's', 'a', 's', 'a', 's']\n",
      "['d', 'c', 's', 'a', 's', 'a', 's']\n",
      "['d', 'c', 's', 'a', 's', 'a', 's']\n",
      "['d', 'c', 's', 'a', 's', 'a', 's']\n",
      "['d', 'c', 's', 'a', 's', 'a', 's']\n",
      "['d', 'c', 's', 'a', 's', 'a', 's']\n",
      "['d', 'c', 's', 'a', 's', 'a', 's']\n",
      "['d', 'c', 's', 'a', 's', 'a', 's']\n",
      "['d', 'c', 's', 'a', 's', 'a', 's']\n",
      "['d', 'c', 's', 'a', 's', 'a', 's']\n",
      "['d', 'c', 's', 'a', 's', 'a', 's']\n",
      "['d', 's', 'a', 'c', 's', 'a', 's']\n",
      "['d', 's', 'a', 'c', 's', 'a', 's']\n",
      "['d', 's', 'a', 's', 'a', 'c', 's']\n",
      "['d', 's', 'a', 's', 'a', 's', 'c']\n",
      "['d', 's', 'a', 's', 'c', 'a', 's']\n",
      "['d', 's', 'a', 's', 'c', 's', 'a']\n",
      "['d', 's', 'a', 's', 'a', 'c', 's']\n",
      "['d', 's', 'a', 's', 'a', 's', 'c']\n",
      "['d', 's', 'a', 's', 'c', 'a', 's']\n",
      "['d', 's', 'a', 's', 'c', 's', 'a']\n",
      "['d', 's', 'a', 'c', 's', 'a', 's']\n",
      "['d', 's', 'a', 'c', 's', 'a', 's']\n",
      "['d', 's', 'a', 's', 'a', 'c', 's']\n",
      "['d', 's', 'a', 's', 'a', 's', 'c']\n",
      "['d', 's', 'a', 's', 'c', 'a', 's']\n",
      "['d', 's', 'a', 's', 'c', 's', 'a']\n",
      "['d', 's', 'a', 's', 'a', 'c', 's']\n",
      "['d', 's', 'a', 's', 'a', 's', 'c']\n",
      "['d', 's', 'a', 's', 'c', 'a', 's']\n",
      "['d', 's', 'a', 's', 'c', 's', 'a']\n",
      "['d', 's', 'c', 'a', 's', 'a', 's']\n",
      "['d', 's', 'c', 'a', 's', 'a', 's']\n",
      "['d', 's', 'c', 'a', 's', 'a', 's']\n",
      "['d', 's', 'c', 'a', 's', 'a', 's']\n",
      "['d', 's', 'c', 's', 'a', 's', 'a']\n",
      "['d', 's', 'c', 's', 'a', 's', 'a']\n",
      "['d', 's', 'c', 's', 'a', 's', 'a']\n",
      "['d', 's', 'c', 's', 'a', 's', 'a']\n",
      "['d', 's', 'a', 'c', 's', 'a', 's']\n",
      "['d', 's', 'a', 'c', 's', 'a', 's']\n",
      "['d', 's', 'a', 's', 'a', 'c', 's']\n",
      "['d', 's', 'a', 's', 'a', 's', 'c']\n",
      "['d', 's', 'a', 's', 'c', 'a', 's']\n",
      "['d', 's', 'a', 's', 'c', 's', 'a']\n",
      "['d', 's', 'a', 's', 'a', 'c', 's']\n",
      "['d', 's', 'a', 's', 'a', 's', 'c']\n",
      "['d', 's', 'a', 's', 'c', 'a', 's']\n",
      "['d', 's', 'a', 's', 'c', 's', 'a']\n",
      "['d', 's', 'a', 'c', 's', 'a', 's']\n",
      "['d', 's', 'a', 'c', 's', 'a', 's']\n",
      "['d', 's', 'a', 's', 'a', 'c', 's']\n",
      "['d', 's', 'a', 's', 'a', 's', 'c']\n",
      "['d', 's', 'a', 's', 'c', 'a', 's']\n",
      "['d', 's', 'a', 's', 'c', 's', 'a']\n",
      "['d', 's', 'a', 's', 'a', 'c', 's']\n",
      "['d', 's', 'a', 's', 'a', 's', 'c']\n",
      "['d', 's', 'a', 's', 'c', 'a', 's']\n",
      "['d', 's', 'a', 's', 'c', 's', 'a']\n",
      "['d', 's', 'c', 'a', 's', 'a', 's']\n",
      "['d', 's', 'c', 'a', 's', 'a', 's']\n",
      "['d', 's', 'c', 'a', 's', 'a', 's']\n",
      "['d', 's', 'c', 'a', 's', 'a', 's']\n",
      "['d', 's', 'c', 's', 'a', 's', 'a']\n",
      "['d', 's', 'c', 's', 'a', 's', 'a']\n",
      "['d', 's', 'c', 's', 'a', 's', 'a']\n",
      "['d', 's', 'c', 's', 'a', 's', 'a']\n",
      "['d', 's', 'a', 'c', 's', 'a', 's']\n",
      "['d', 's', 'a', 'c', 's', 'a', 's']\n",
      "['d', 's', 'a', 's', 'a', 'c', 's']\n",
      "['d', 's', 'a', 's', 'a', 's', 'c']\n",
      "['d', 's', 'a', 's', 'c', 'a', 's']\n",
      "['d', 's', 'a', 's', 'c', 's', 'a']\n",
      "['d', 's', 'a', 's', 'a', 'c', 's']\n",
      "['d', 's', 'a', 's', 'a', 's', 'c']\n",
      "['d', 's', 'a', 's', 'c', 'a', 's']\n",
      "['d', 's', 'a', 's', 'c', 's', 'a']\n",
      "['d', 's', 'a', 'c', 's', 'a', 's']\n",
      "['d', 's', 'a', 'c', 's', 'a', 's']\n",
      "['d', 's', 'a', 's', 'a', 'c', 's']\n",
      "['d', 's', 'a', 's', 'a', 's', 'c']\n",
      "['d', 's', 'a', 's', 'c', 'a', 's']\n",
      "['d', 's', 'a', 's', 'c', 's', 'a']\n",
      "['d', 's', 'a', 's', 'a', 'c', 's']\n",
      "['d', 's', 'a', 's', 'a', 's', 'c']\n",
      "['d', 's', 'a', 's', 'c', 'a', 's']\n",
      "['d', 's', 'a', 's', 'c', 's', 'a']\n",
      "['d', 's', 'c', 'a', 's', 'a', 's']\n",
      "['d', 's', 'c', 'a', 's', 'a', 's']\n",
      "['d', 's', 'c', 'a', 's', 'a', 's']\n",
      "['d', 's', 'c', 'a', 's', 'a', 's']\n",
      "['d', 's', 'c', 's', 'a', 's', 'a']\n",
      "['d', 's', 'c', 's', 'a', 's', 'a']\n",
      "['d', 's', 'c', 's', 'a', 's', 'a']\n",
      "['d', 's', 'c', 's', 'a', 's', 'a']\n",
      "['s', 'a', 'c', 'a', 's', 'd', 's']\n",
      "['s', 'a', 'c', 'a', 's', 'd', 's']\n",
      "['s', 'a', 'c', 'd', 's', 'a', 's']\n",
      "['s', 'a', 'c', 'd', 's', 'a', 's']\n",
      "['s', 'a', 'c', 's', 'a', 'd', 's']\n",
      "['s', 'a', 'c', 's', 'a', 's', 'd']\n",
      "['s', 'a', 'c', 's', 'd', 'a', 's']\n",
      "['s', 'a', 'c', 's', 'd', 's', 'a']\n",
      "['s', 'a', 'c', 's', 'a', 'd', 's']\n",
      "['s', 'a', 'c', 's', 'a', 's', 'd']\n",
      "['s', 'a', 'c', 's', 'd', 'a', 's']\n",
      "['s', 'a', 'c', 's', 'd', 's', 'a']\n",
      "['s', 'a', 'd', 'a', 's', 'c', 's']\n",
      "['s', 'a', 'd', 'a', 's', 'c', 's']\n",
      "['s', 'a', 'd', 'c', 's', 'a', 's']\n",
      "['s', 'a', 'd', 'c', 's', 'a', 's']\n",
      "['s', 'a', 'd', 's', 'a', 'c', 's']\n",
      "['s', 'a', 'd', 's', 'a', 's', 'c']\n",
      "['s', 'a', 'd', 's', 'c', 'a', 's']\n",
      "['s', 'a', 'd', 's', 'c', 's', 'a']\n",
      "['s', 'a', 'd', 's', 'a', 'c', 's']\n",
      "['s', 'a', 'd', 's', 'a', 's', 'c']\n",
      "['s', 'a', 'd', 's', 'c', 'a', 's']\n",
      "['s', 'a', 'd', 's', 'c', 's', 'a']\n",
      "['s', 'a', 's', 'a', 'c', 'd', 's']\n",
      "['s', 'a', 's', 'a', 'c', 's', 'd']\n",
      "['s', 'a', 's', 'a', 'd', 'c', 's']\n",
      "['s', 'a', 's', 'a', 'd', 's', 'c']\n",
      "['s', 'a', 's', 'a', 's', 'c', 'd']\n",
      "['s', 'a', 's', 'a', 's', 'd', 'c']\n",
      "['s', 'a', 's', 'c', 'a', 'd', 's']\n",
      "['s', 'a', 's', 'c', 'a', 's', 'd']\n",
      "['s', 'a', 's', 'c', 'd', 'a', 's']\n",
      "['s', 'a', 's', 'c', 'd', 's', 'a']\n",
      "['s', 'a', 's', 'c', 's', 'a', 'd']\n",
      "['s', 'a', 's', 'c', 's', 'd', 'a']\n",
      "['s', 'a', 's', 'd', 'a', 'c', 's']\n",
      "['s', 'a', 's', 'd', 'a', 's', 'c']\n",
      "['s', 'a', 's', 'd', 'c', 'a', 's']\n",
      "['s', 'a', 's', 'd', 'c', 's', 'a']\n",
      "['s', 'a', 's', 'd', 's', 'a', 'c']\n",
      "['s', 'a', 's', 'd', 's', 'c', 'a']\n",
      "['s', 'a', 's', 'a', 'c', 'd', 's']\n",
      "['s', 'a', 's', 'a', 'c', 's', 'd']\n",
      "['s', 'a', 's', 'a', 'd', 'c', 's']\n",
      "['s', 'a', 's', 'a', 'd', 's', 'c']\n",
      "['s', 'a', 's', 'a', 's', 'c', 'd']\n",
      "['s', 'a', 's', 'a', 's', 'd', 'c']\n",
      "['s', 'a', 's', 'c', 'a', 'd', 's']\n",
      "['s', 'a', 's', 'c', 'a', 's', 'd']\n",
      "['s', 'a', 's', 'c', 'd', 'a', 's']\n",
      "['s', 'a', 's', 'c', 'd', 's', 'a']\n",
      "['s', 'a', 's', 'c', 's', 'a', 'd']\n",
      "['s', 'a', 's', 'c', 's', 'd', 'a']\n",
      "['s', 'a', 's', 'd', 'a', 'c', 's']\n",
      "['s', 'a', 's', 'd', 'a', 's', 'c']\n",
      "['s', 'a', 's', 'd', 'c', 'a', 's']\n",
      "['s', 'a', 's', 'd', 'c', 's', 'a']\n",
      "['s', 'a', 's', 'd', 's', 'a', 'c']\n",
      "['s', 'a', 's', 'd', 's', 'c', 'a']\n",
      "['s', 'a', 'c', 'a', 's', 'd', 's']\n",
      "['s', 'a', 'c', 'a', 's', 'd', 's']\n",
      "['s', 'a', 'c', 'd', 's', 'a', 's']\n",
      "['s', 'a', 'c', 'd', 's', 'a', 's']\n",
      "['s', 'a', 'c', 's', 'a', 'd', 's']\n",
      "['s', 'a', 'c', 's', 'a', 's', 'd']\n",
      "['s', 'a', 'c', 's', 'd', 'a', 's']\n",
      "['s', 'a', 'c', 's', 'd', 's', 'a']\n",
      "['s', 'a', 'c', 's', 'a', 'd', 's']\n",
      "['s', 'a', 'c', 's', 'a', 's', 'd']\n",
      "['s', 'a', 'c', 's', 'd', 'a', 's']\n",
      "['s', 'a', 'c', 's', 'd', 's', 'a']\n",
      "['s', 'a', 'd', 'a', 's', 'c', 's']\n",
      "['s', 'a', 'd', 'a', 's', 'c', 's']\n",
      "['s', 'a', 'd', 'c', 's', 'a', 's']\n",
      "['s', 'a', 'd', 'c', 's', 'a', 's']\n",
      "['s', 'a', 'd', 's', 'a', 'c', 's']\n",
      "['s', 'a', 'd', 's', 'a', 's', 'c']\n",
      "['s', 'a', 'd', 's', 'c', 'a', 's']\n",
      "['s', 'a', 'd', 's', 'c', 's', 'a']\n",
      "['s', 'a', 'd', 's', 'a', 'c', 's']\n",
      "['s', 'a', 'd', 's', 'a', 's', 'c']\n",
      "['s', 'a', 'd', 's', 'c', 'a', 's']\n",
      "['s', 'a', 'd', 's', 'c', 's', 'a']\n",
      "['s', 'a', 's', 'a', 'c', 'd', 's']\n",
      "['s', 'a', 's', 'a', 'c', 's', 'd']\n",
      "['s', 'a', 's', 'a', 'd', 'c', 's']\n",
      "['s', 'a', 's', 'a', 'd', 's', 'c']\n",
      "['s', 'a', 's', 'a', 's', 'c', 'd']\n",
      "['s', 'a', 's', 'a', 's', 'd', 'c']\n",
      "['s', 'a', 's', 'c', 'a', 'd', 's']\n",
      "['s', 'a', 's', 'c', 'a', 's', 'd']\n",
      "['s', 'a', 's', 'c', 'd', 'a', 's']\n",
      "['s', 'a', 's', 'c', 'd', 's', 'a']\n",
      "['s', 'a', 's', 'c', 's', 'a', 'd']\n",
      "['s', 'a', 's', 'c', 's', 'd', 'a']\n",
      "['s', 'a', 's', 'd', 'a', 'c', 's']\n",
      "['s', 'a', 's', 'd', 'a', 's', 'c']\n",
      "['s', 'a', 's', 'd', 'c', 'a', 's']\n",
      "['s', 'a', 's', 'd', 'c', 's', 'a']\n",
      "['s', 'a', 's', 'd', 's', 'a', 'c']\n",
      "['s', 'a', 's', 'd', 's', 'c', 'a']\n",
      "['s', 'a', 's', 'a', 'c', 'd', 's']\n",
      "['s', 'a', 's', 'a', 'c', 's', 'd']\n",
      "['s', 'a', 's', 'a', 'd', 'c', 's']\n",
      "['s', 'a', 's', 'a', 'd', 's', 'c']\n",
      "['s', 'a', 's', 'a', 's', 'c', 'd']\n",
      "['s', 'a', 's', 'a', 's', 'd', 'c']\n",
      "['s', 'a', 's', 'c', 'a', 'd', 's']\n",
      "['s', 'a', 's', 'c', 'a', 's', 'd']\n",
      "['s', 'a', 's', 'c', 'd', 'a', 's']\n",
      "['s', 'a', 's', 'c', 'd', 's', 'a']\n",
      "['s', 'a', 's', 'c', 's', 'a', 'd']\n",
      "['s', 'a', 's', 'c', 's', 'd', 'a']\n",
      "['s', 'a', 's', 'd', 'a', 'c', 's']\n",
      "['s', 'a', 's', 'd', 'a', 's', 'c']\n",
      "['s', 'a', 's', 'd', 'c', 'a', 's']\n",
      "['s', 'a', 's', 'd', 'c', 's', 'a']\n",
      "['s', 'a', 's', 'd', 's', 'a', 'c']\n",
      "['s', 'a', 's', 'd', 's', 'c', 'a']\n",
      "['s', 'c', 'a', 'd', 's', 'a', 's']\n",
      "['s', 'c', 'a', 'd', 's', 'a', 's']\n",
      "['s', 'c', 'a', 's', 'a', 'd', 's']\n",
      "['s', 'c', 'a', 's', 'a', 's', 'd']\n",
      "['s', 'c', 'a', 's', 'd', 'a', 's']\n",
      "['s', 'c', 'a', 's', 'd', 's', 'a']\n",
      "['s', 'c', 'a', 's', 'a', 'd', 's']\n",
      "['s', 'c', 'a', 's', 'a', 's', 'd']\n",
      "['s', 'c', 'a', 's', 'd', 'a', 's']\n",
      "['s', 'c', 'a', 's', 'd', 's', 'a']\n",
      "['s', 'c', 'a', 'd', 's', 'a', 's']\n",
      "['s', 'c', 'a', 'd', 's', 'a', 's']\n",
      "['s', 'c', 'a', 's', 'a', 'd', 's']\n",
      "['s', 'c', 'a', 's', 'a', 's', 'd']\n",
      "['s', 'c', 'a', 's', 'd', 'a', 's']\n",
      "['s', 'c', 'a', 's', 'd', 's', 'a']\n",
      "['s', 'c', 'a', 's', 'a', 'd', 's']\n",
      "['s', 'c', 'a', 's', 'a', 's', 'd']\n",
      "['s', 'c', 'a', 's', 'd', 'a', 's']\n",
      "['s', 'c', 'a', 's', 'd', 's', 'a']\n",
      "['s', 'c', 'd', 'a', 's', 'a', 's']\n",
      "['s', 'c', 'd', 'a', 's', 'a', 's']\n",
      "['s', 'c', 'd', 'a', 's', 'a', 's']\n",
      "['s', 'c', 'd', 'a', 's', 'a', 's']\n",
      "['s', 'c', 'd', 's', 'a', 's', 'a']\n",
      "['s', 'c', 'd', 's', 'a', 's', 'a']\n",
      "['s', 'c', 'd', 's', 'a', 's', 'a']\n",
      "['s', 'c', 'd', 's', 'a', 's', 'a']\n",
      "['s', 'c', 's', 'a', 'd', 'a', 's']\n",
      "['s', 'c', 's', 'a', 'd', 's', 'a']\n",
      "['s', 'c', 's', 'a', 's', 'a', 'd']\n",
      "['s', 'c', 's', 'a', 's', 'd', 'a']\n",
      "['s', 'c', 's', 'a', 'd', 'a', 's']\n",
      "['s', 'c', 's', 'a', 'd', 's', 'a']\n",
      "['s', 'c', 's', 'a', 's', 'a', 'd']\n",
      "['s', 'c', 's', 'a', 's', 'd', 'a']\n",
      "['s', 'c', 's', 'd', 'a', 's', 'a']\n",
      "['s', 'c', 's', 'd', 'a', 's', 'a']\n",
      "['s', 'c', 's', 'a', 'd', 'a', 's']\n",
      "['s', 'c', 's', 'a', 'd', 's', 'a']\n",
      "['s', 'c', 's', 'a', 's', 'a', 'd']\n",
      "['s', 'c', 's', 'a', 's', 'd', 'a']\n",
      "['s', 'c', 's', 'a', 'd', 'a', 's']\n",
      "['s', 'c', 's', 'a', 'd', 's', 'a']\n",
      "['s', 'c', 's', 'a', 's', 'a', 'd']\n",
      "['s', 'c', 's', 'a', 's', 'd', 'a']\n",
      "['s', 'c', 's', 'd', 'a', 's', 'a']\n",
      "['s', 'c', 's', 'd', 'a', 's', 'a']\n",
      "['s', 'd', 'a', 'c', 's', 'a', 's']\n",
      "['s', 'd', 'a', 'c', 's', 'a', 's']\n",
      "['s', 'd', 'a', 's', 'a', 'c', 's']\n",
      "['s', 'd', 'a', 's', 'a', 's', 'c']\n",
      "['s', 'd', 'a', 's', 'c', 'a', 's']\n",
      "['s', 'd', 'a', 's', 'c', 's', 'a']\n",
      "['s', 'd', 'a', 's', 'a', 'c', 's']\n",
      "['s', 'd', 'a', 's', 'a', 's', 'c']\n",
      "['s', 'd', 'a', 's', 'c', 'a', 's']\n",
      "['s', 'd', 'a', 's', 'c', 's', 'a']\n",
      "['s', 'd', 'a', 'c', 's', 'a', 's']\n",
      "['s', 'd', 'a', 'c', 's', 'a', 's']\n",
      "['s', 'd', 'a', 's', 'a', 'c', 's']\n",
      "['s', 'd', 'a', 's', 'a', 's', 'c']\n",
      "['s', 'd', 'a', 's', 'c', 'a', 's']\n",
      "['s', 'd', 'a', 's', 'c', 's', 'a']\n",
      "['s', 'd', 'a', 's', 'a', 'c', 's']\n",
      "['s', 'd', 'a', 's', 'a', 's', 'c']\n",
      "['s', 'd', 'a', 's', 'c', 'a', 's']\n",
      "['s', 'd', 'a', 's', 'c', 's', 'a']\n",
      "['s', 'd', 'c', 'a', 's', 'a', 's']\n",
      "['s', 'd', 'c', 'a', 's', 'a', 's']\n",
      "['s', 'd', 'c', 'a', 's', 'a', 's']\n",
      "['s', 'd', 'c', 'a', 's', 'a', 's']\n",
      "['s', 'd', 'c', 's', 'a', 's', 'a']\n",
      "['s', 'd', 'c', 's', 'a', 's', 'a']\n",
      "['s', 'd', 'c', 's', 'a', 's', 'a']\n",
      "['s', 'd', 'c', 's', 'a', 's', 'a']\n",
      "['s', 'd', 's', 'a', 'c', 'a', 's']\n",
      "['s', 'd', 's', 'a', 'c', 's', 'a']\n",
      "['s', 'd', 's', 'a', 's', 'a', 'c']\n",
      "['s', 'd', 's', 'a', 's', 'c', 'a']\n",
      "['s', 'd', 's', 'a', 'c', 'a', 's']\n",
      "['s', 'd', 's', 'a', 'c', 's', 'a']\n",
      "['s', 'd', 's', 'a', 's', 'a', 'c']\n",
      "['s', 'd', 's', 'a', 's', 'c', 'a']\n",
      "['s', 'd', 's', 'c', 'a', 's', 'a']\n",
      "['s', 'd', 's', 'c', 'a', 's', 'a']\n",
      "['s', 'd', 's', 'a', 'c', 'a', 's']\n",
      "['s', 'd', 's', 'a', 'c', 's', 'a']\n",
      "['s', 'd', 's', 'a', 's', 'a', 'c']\n",
      "['s', 'd', 's', 'a', 's', 'c', 'a']\n",
      "['s', 'd', 's', 'a', 'c', 'a', 's']\n",
      "['s', 'd', 's', 'a', 'c', 's', 'a']\n",
      "['s', 'd', 's', 'a', 's', 'a', 'c']\n",
      "['s', 'd', 's', 'a', 's', 'c', 'a']\n",
      "['s', 'd', 's', 'c', 'a', 's', 'a']\n",
      "['s', 'd', 's', 'c', 'a', 's', 'a']\n",
      "['s', 'a', 'c', 'a', 's', 'd', 's']\n",
      "['s', 'a', 'c', 'a', 's', 'd', 's']\n",
      "['s', 'a', 'c', 'd', 's', 'a', 's']\n",
      "['s', 'a', 'c', 'd', 's', 'a', 's']\n",
      "['s', 'a', 'c', 's', 'a', 'd', 's']\n",
      "['s', 'a', 'c', 's', 'a', 's', 'd']\n",
      "['s', 'a', 'c', 's', 'd', 'a', 's']\n",
      "['s', 'a', 'c', 's', 'd', 's', 'a']\n",
      "['s', 'a', 'c', 's', 'a', 'd', 's']\n",
      "['s', 'a', 'c', 's', 'a', 's', 'd']\n",
      "['s', 'a', 'c', 's', 'd', 'a', 's']\n",
      "['s', 'a', 'c', 's', 'd', 's', 'a']\n",
      "['s', 'a', 'd', 'a', 's', 'c', 's']\n",
      "['s', 'a', 'd', 'a', 's', 'c', 's']\n",
      "['s', 'a', 'd', 'c', 's', 'a', 's']\n",
      "['s', 'a', 'd', 'c', 's', 'a', 's']\n",
      "['s', 'a', 'd', 's', 'a', 'c', 's']\n",
      "['s', 'a', 'd', 's', 'a', 's', 'c']\n",
      "['s', 'a', 'd', 's', 'c', 'a', 's']\n",
      "['s', 'a', 'd', 's', 'c', 's', 'a']\n",
      "['s', 'a', 'd', 's', 'a', 'c', 's']\n",
      "['s', 'a', 'd', 's', 'a', 's', 'c']\n",
      "['s', 'a', 'd', 's', 'c', 'a', 's']\n",
      "['s', 'a', 'd', 's', 'c', 's', 'a']\n",
      "['s', 'a', 's', 'a', 'c', 'd', 's']\n",
      "['s', 'a', 's', 'a', 'c', 's', 'd']\n",
      "['s', 'a', 's', 'a', 'd', 'c', 's']\n",
      "['s', 'a', 's', 'a', 'd', 's', 'c']\n",
      "['s', 'a', 's', 'a', 's', 'c', 'd']\n",
      "['s', 'a', 's', 'a', 's', 'd', 'c']\n",
      "['s', 'a', 's', 'c', 'a', 'd', 's']\n",
      "['s', 'a', 's', 'c', 'a', 's', 'd']\n",
      "['s', 'a', 's', 'c', 'd', 'a', 's']\n",
      "['s', 'a', 's', 'c', 'd', 's', 'a']\n",
      "['s', 'a', 's', 'c', 's', 'a', 'd']\n",
      "['s', 'a', 's', 'c', 's', 'd', 'a']\n",
      "['s', 'a', 's', 'd', 'a', 'c', 's']\n",
      "['s', 'a', 's', 'd', 'a', 's', 'c']\n",
      "['s', 'a', 's', 'd', 'c', 'a', 's']\n",
      "['s', 'a', 's', 'd', 'c', 's', 'a']\n",
      "['s', 'a', 's', 'd', 's', 'a', 'c']\n",
      "['s', 'a', 's', 'd', 's', 'c', 'a']\n",
      "['s', 'a', 's', 'a', 'c', 'd', 's']\n",
      "['s', 'a', 's', 'a', 'c', 's', 'd']\n",
      "['s', 'a', 's', 'a', 'd', 'c', 's']\n",
      "['s', 'a', 's', 'a', 'd', 's', 'c']\n",
      "['s', 'a', 's', 'a', 's', 'c', 'd']\n",
      "['s', 'a', 's', 'a', 's', 'd', 'c']\n",
      "['s', 'a', 's', 'c', 'a', 'd', 's']\n",
      "['s', 'a', 's', 'c', 'a', 's', 'd']\n",
      "['s', 'a', 's', 'c', 'd', 'a', 's']\n",
      "['s', 'a', 's', 'c', 'd', 's', 'a']\n",
      "['s', 'a', 's', 'c', 's', 'a', 'd']\n",
      "['s', 'a', 's', 'c', 's', 'd', 'a']\n",
      "['s', 'a', 's', 'd', 'a', 'c', 's']\n",
      "['s', 'a', 's', 'd', 'a', 's', 'c']\n",
      "['s', 'a', 's', 'd', 'c', 'a', 's']\n",
      "['s', 'a', 's', 'd', 'c', 's', 'a']\n",
      "['s', 'a', 's', 'd', 's', 'a', 'c']\n",
      "['s', 'a', 's', 'd', 's', 'c', 'a']\n",
      "['s', 'a', 'c', 'a', 's', 'd', 's']\n",
      "['s', 'a', 'c', 'a', 's', 'd', 's']\n",
      "['s', 'a', 'c', 'd', 's', 'a', 's']\n",
      "['s', 'a', 'c', 'd', 's', 'a', 's']\n",
      "['s', 'a', 'c', 's', 'a', 'd', 's']\n",
      "['s', 'a', 'c', 's', 'a', 's', 'd']\n",
      "['s', 'a', 'c', 's', 'd', 'a', 's']\n",
      "['s', 'a', 'c', 's', 'd', 's', 'a']\n",
      "['s', 'a', 'c', 's', 'a', 'd', 's']\n",
      "['s', 'a', 'c', 's', 'a', 's', 'd']\n",
      "['s', 'a', 'c', 's', 'd', 'a', 's']\n",
      "['s', 'a', 'c', 's', 'd', 's', 'a']\n",
      "['s', 'a', 'd', 'a', 's', 'c', 's']\n",
      "['s', 'a', 'd', 'a', 's', 'c', 's']\n",
      "['s', 'a', 'd', 'c', 's', 'a', 's']\n",
      "['s', 'a', 'd', 'c', 's', 'a', 's']\n",
      "['s', 'a', 'd', 's', 'a', 'c', 's']\n",
      "['s', 'a', 'd', 's', 'a', 's', 'c']\n",
      "['s', 'a', 'd', 's', 'c', 'a', 's']\n",
      "['s', 'a', 'd', 's', 'c', 's', 'a']\n",
      "['s', 'a', 'd', 's', 'a', 'c', 's']\n",
      "['s', 'a', 'd', 's', 'a', 's', 'c']\n",
      "['s', 'a', 'd', 's', 'c', 'a', 's']\n",
      "['s', 'a', 'd', 's', 'c', 's', 'a']\n",
      "['s', 'a', 's', 'a', 'c', 'd', 's']\n",
      "['s', 'a', 's', 'a', 'c', 's', 'd']\n",
      "['s', 'a', 's', 'a', 'd', 'c', 's']\n",
      "['s', 'a', 's', 'a', 'd', 's', 'c']\n",
      "['s', 'a', 's', 'a', 's', 'c', 'd']\n",
      "['s', 'a', 's', 'a', 's', 'd', 'c']\n",
      "['s', 'a', 's', 'c', 'a', 'd', 's']\n",
      "['s', 'a', 's', 'c', 'a', 's', 'd']\n",
      "['s', 'a', 's', 'c', 'd', 'a', 's']\n",
      "['s', 'a', 's', 'c', 'd', 's', 'a']\n",
      "['s', 'a', 's', 'c', 's', 'a', 'd']\n",
      "['s', 'a', 's', 'c', 's', 'd', 'a']\n",
      "['s', 'a', 's', 'd', 'a', 'c', 's']\n",
      "['s', 'a', 's', 'd', 'a', 's', 'c']\n",
      "['s', 'a', 's', 'd', 'c', 'a', 's']\n",
      "['s', 'a', 's', 'd', 'c', 's', 'a']\n",
      "['s', 'a', 's', 'd', 's', 'a', 'c']\n",
      "['s', 'a', 's', 'd', 's', 'c', 'a']\n",
      "['s', 'a', 's', 'a', 'c', 'd', 's']\n",
      "['s', 'a', 's', 'a', 'c', 's', 'd']\n",
      "['s', 'a', 's', 'a', 'd', 'c', 's']\n",
      "['s', 'a', 's', 'a', 'd', 's', 'c']\n",
      "['s', 'a', 's', 'a', 's', 'c', 'd']\n",
      "['s', 'a', 's', 'a', 's', 'd', 'c']\n",
      "['s', 'a', 's', 'c', 'a', 'd', 's']\n",
      "['s', 'a', 's', 'c', 'a', 's', 'd']\n",
      "['s', 'a', 's', 'c', 'd', 'a', 's']\n",
      "['s', 'a', 's', 'c', 'd', 's', 'a']\n",
      "['s', 'a', 's', 'c', 's', 'a', 'd']\n",
      "['s', 'a', 's', 'c', 's', 'd', 'a']\n",
      "['s', 'a', 's', 'd', 'a', 'c', 's']\n",
      "['s', 'a', 's', 'd', 'a', 's', 'c']\n",
      "['s', 'a', 's', 'd', 'c', 'a', 's']\n",
      "['s', 'a', 's', 'd', 'c', 's', 'a']\n",
      "['s', 'a', 's', 'd', 's', 'a', 'c']\n",
      "['s', 'a', 's', 'd', 's', 'c', 'a']\n",
      "['s', 'c', 'a', 'd', 's', 'a', 's']\n",
      "['s', 'c', 'a', 'd', 's', 'a', 's']\n",
      "['s', 'c', 'a', 's', 'a', 'd', 's']\n",
      "['s', 'c', 'a', 's', 'a', 's', 'd']\n",
      "['s', 'c', 'a', 's', 'd', 'a', 's']\n",
      "['s', 'c', 'a', 's', 'd', 's', 'a']\n",
      "['s', 'c', 'a', 's', 'a', 'd', 's']\n",
      "['s', 'c', 'a', 's', 'a', 's', 'd']\n",
      "['s', 'c', 'a', 's', 'd', 'a', 's']\n",
      "['s', 'c', 'a', 's', 'd', 's', 'a']\n",
      "['s', 'c', 'a', 'd', 's', 'a', 's']\n",
      "['s', 'c', 'a', 'd', 's', 'a', 's']\n",
      "['s', 'c', 'a', 's', 'a', 'd', 's']\n",
      "['s', 'c', 'a', 's', 'a', 's', 'd']\n",
      "['s', 'c', 'a', 's', 'd', 'a', 's']\n",
      "['s', 'c', 'a', 's', 'd', 's', 'a']\n",
      "['s', 'c', 'a', 's', 'a', 'd', 's']\n",
      "['s', 'c', 'a', 's', 'a', 's', 'd']\n",
      "['s', 'c', 'a', 's', 'd', 'a', 's']\n",
      "['s', 'c', 'a', 's', 'd', 's', 'a']\n",
      "['s', 'c', 'd', 'a', 's', 'a', 's']\n",
      "['s', 'c', 'd', 'a', 's', 'a', 's']\n",
      "['s', 'c', 'd', 'a', 's', 'a', 's']\n",
      "['s', 'c', 'd', 'a', 's', 'a', 's']\n",
      "['s', 'c', 'd', 's', 'a', 's', 'a']\n",
      "['s', 'c', 'd', 's', 'a', 's', 'a']\n",
      "['s', 'c', 'd', 's', 'a', 's', 'a']\n",
      "['s', 'c', 'd', 's', 'a', 's', 'a']\n",
      "['s', 'c', 's', 'a', 'd', 'a', 's']\n",
      "['s', 'c', 's', 'a', 'd', 's', 'a']\n",
      "['s', 'c', 's', 'a', 's', 'a', 'd']\n",
      "['s', 'c', 's', 'a', 's', 'd', 'a']\n",
      "['s', 'c', 's', 'a', 'd', 'a', 's']\n",
      "['s', 'c', 's', 'a', 'd', 's', 'a']\n",
      "['s', 'c', 's', 'a', 's', 'a', 'd']\n",
      "['s', 'c', 's', 'a', 's', 'd', 'a']\n",
      "['s', 'c', 's', 'd', 'a', 's', 'a']\n",
      "['s', 'c', 's', 'd', 'a', 's', 'a']\n",
      "['s', 'c', 's', 'a', 'd', 'a', 's']\n",
      "['s', 'c', 's', 'a', 'd', 's', 'a']\n",
      "['s', 'c', 's', 'a', 's', 'a', 'd']\n",
      "['s', 'c', 's', 'a', 's', 'd', 'a']\n",
      "['s', 'c', 's', 'a', 'd', 'a', 's']\n",
      "['s', 'c', 's', 'a', 'd', 's', 'a']\n",
      "['s', 'c', 's', 'a', 's', 'a', 'd']\n",
      "['s', 'c', 's', 'a', 's', 'd', 'a']\n",
      "['s', 'c', 's', 'd', 'a', 's', 'a']\n",
      "['s', 'c', 's', 'd', 'a', 's', 'a']\n",
      "['s', 'd', 'a', 'c', 's', 'a', 's']\n",
      "['s', 'd', 'a', 'c', 's', 'a', 's']\n",
      "['s', 'd', 'a', 's', 'a', 'c', 's']\n",
      "['s', 'd', 'a', 's', 'a', 's', 'c']\n",
      "['s', 'd', 'a', 's', 'c', 'a', 's']\n",
      "['s', 'd', 'a', 's', 'c', 's', 'a']\n",
      "['s', 'd', 'a', 's', 'a', 'c', 's']\n",
      "['s', 'd', 'a', 's', 'a', 's', 'c']\n",
      "['s', 'd', 'a', 's', 'c', 'a', 's']\n",
      "['s', 'd', 'a', 's', 'c', 's', 'a']\n",
      "['s', 'd', 'a', 'c', 's', 'a', 's']\n",
      "['s', 'd', 'a', 'c', 's', 'a', 's']\n",
      "['s', 'd', 'a', 's', 'a', 'c', 's']\n",
      "['s', 'd', 'a', 's', 'a', 's', 'c']\n",
      "['s', 'd', 'a', 's', 'c', 'a', 's']\n",
      "['s', 'd', 'a', 's', 'c', 's', 'a']\n",
      "['s', 'd', 'a', 's', 'a', 'c', 's']\n",
      "['s', 'd', 'a', 's', 'a', 's', 'c']\n",
      "['s', 'd', 'a', 's', 'c', 'a', 's']\n",
      "['s', 'd', 'a', 's', 'c', 's', 'a']\n",
      "['s', 'd', 'c', 'a', 's', 'a', 's']\n",
      "['s', 'd', 'c', 'a', 's', 'a', 's']\n",
      "['s', 'd', 'c', 'a', 's', 'a', 's']\n",
      "['s', 'd', 'c', 'a', 's', 'a', 's']\n",
      "['s', 'd', 'c', 's', 'a', 's', 'a']\n",
      "['s', 'd', 'c', 's', 'a', 's', 'a']\n",
      "['s', 'd', 'c', 's', 'a', 's', 'a']\n",
      "['s', 'd', 'c', 's', 'a', 's', 'a']\n",
      "['s', 'd', 's', 'a', 'c', 'a', 's']\n",
      "['s', 'd', 's', 'a', 'c', 's', 'a']\n",
      "['s', 'd', 's', 'a', 's', 'a', 'c']\n",
      "['s', 'd', 's', 'a', 's', 'c', 'a']\n",
      "['s', 'd', 's', 'a', 'c', 'a', 's']\n",
      "['s', 'd', 's', 'a', 'c', 's', 'a']\n",
      "['s', 'd', 's', 'a', 's', 'a', 'c']\n",
      "['s', 'd', 's', 'a', 's', 'c', 'a']\n",
      "['s', 'd', 's', 'c', 'a', 's', 'a']\n",
      "['s', 'd', 's', 'c', 'a', 's', 'a']\n",
      "['s', 'd', 's', 'a', 'c', 'a', 's']\n",
      "['s', 'd', 's', 'a', 'c', 's', 'a']\n",
      "['s', 'd', 's', 'a', 's', 'a', 'c']\n",
      "['s', 'd', 's', 'a', 's', 'c', 'a']\n",
      "['s', 'd', 's', 'a', 'c', 'a', 's']\n",
      "['s', 'd', 's', 'a', 'c', 's', 'a']\n",
      "['s', 'd', 's', 'a', 's', 'a', 'c']\n",
      "['s', 'd', 's', 'a', 's', 'c', 'a']\n",
      "['s', 'd', 's', 'c', 'a', 's', 'a']\n",
      "['s', 'd', 's', 'c', 'a', 's', 'a']\n",
      "['s', 'a', 'c', 'a', 's', 'd', 's']\n",
      "['s', 'a', 'c', 'a', 's', 'd', 's']\n",
      "['s', 'a', 'c', 'd', 's', 'a', 's']\n",
      "['s', 'a', 'c', 'd', 's', 'a', 's']\n",
      "['s', 'a', 'c', 's', 'a', 'd', 's']\n",
      "['s', 'a', 'c', 's', 'a', 's', 'd']\n",
      "['s', 'a', 'c', 's', 'd', 'a', 's']\n",
      "['s', 'a', 'c', 's', 'd', 's', 'a']\n",
      "['s', 'a', 'c', 's', 'a', 'd', 's']\n",
      "['s', 'a', 'c', 's', 'a', 's', 'd']\n",
      "['s', 'a', 'c', 's', 'd', 'a', 's']\n",
      "['s', 'a', 'c', 's', 'd', 's', 'a']\n",
      "['s', 'a', 'd', 'a', 's', 'c', 's']\n",
      "['s', 'a', 'd', 'a', 's', 'c', 's']\n",
      "['s', 'a', 'd', 'c', 's', 'a', 's']\n",
      "['s', 'a', 'd', 'c', 's', 'a', 's']\n",
      "['s', 'a', 'd', 's', 'a', 'c', 's']\n",
      "['s', 'a', 'd', 's', 'a', 's', 'c']\n",
      "['s', 'a', 'd', 's', 'c', 'a', 's']\n",
      "['s', 'a', 'd', 's', 'c', 's', 'a']\n",
      "['s', 'a', 'd', 's', 'a', 'c', 's']\n",
      "['s', 'a', 'd', 's', 'a', 's', 'c']\n",
      "['s', 'a', 'd', 's', 'c', 'a', 's']\n",
      "['s', 'a', 'd', 's', 'c', 's', 'a']\n",
      "['s', 'a', 's', 'a', 'c', 'd', 's']\n",
      "['s', 'a', 's', 'a', 'c', 's', 'd']\n",
      "['s', 'a', 's', 'a', 'd', 'c', 's']\n",
      "['s', 'a', 's', 'a', 'd', 's', 'c']\n",
      "['s', 'a', 's', 'a', 's', 'c', 'd']\n",
      "['s', 'a', 's', 'a', 's', 'd', 'c']\n",
      "['s', 'a', 's', 'c', 'a', 'd', 's']\n",
      "['s', 'a', 's', 'c', 'a', 's', 'd']\n",
      "['s', 'a', 's', 'c', 'd', 'a', 's']\n",
      "['s', 'a', 's', 'c', 'd', 's', 'a']\n",
      "['s', 'a', 's', 'c', 's', 'a', 'd']\n",
      "['s', 'a', 's', 'c', 's', 'd', 'a']\n",
      "['s', 'a', 's', 'd', 'a', 'c', 's']\n",
      "['s', 'a', 's', 'd', 'a', 's', 'c']\n",
      "['s', 'a', 's', 'd', 'c', 'a', 's']\n",
      "['s', 'a', 's', 'd', 'c', 's', 'a']\n",
      "['s', 'a', 's', 'd', 's', 'a', 'c']\n",
      "['s', 'a', 's', 'd', 's', 'c', 'a']\n",
      "['s', 'a', 's', 'a', 'c', 'd', 's']\n",
      "['s', 'a', 's', 'a', 'c', 's', 'd']\n",
      "['s', 'a', 's', 'a', 'd', 'c', 's']\n",
      "['s', 'a', 's', 'a', 'd', 's', 'c']\n",
      "['s', 'a', 's', 'a', 's', 'c', 'd']\n",
      "['s', 'a', 's', 'a', 's', 'd', 'c']\n",
      "['s', 'a', 's', 'c', 'a', 'd', 's']\n",
      "['s', 'a', 's', 'c', 'a', 's', 'd']\n",
      "['s', 'a', 's', 'c', 'd', 'a', 's']\n",
      "['s', 'a', 's', 'c', 'd', 's', 'a']\n",
      "['s', 'a', 's', 'c', 's', 'a', 'd']\n",
      "['s', 'a', 's', 'c', 's', 'd', 'a']\n",
      "['s', 'a', 's', 'd', 'a', 'c', 's']\n",
      "['s', 'a', 's', 'd', 'a', 's', 'c']\n",
      "['s', 'a', 's', 'd', 'c', 'a', 's']\n",
      "['s', 'a', 's', 'd', 'c', 's', 'a']\n",
      "['s', 'a', 's', 'd', 's', 'a', 'c']\n",
      "['s', 'a', 's', 'd', 's', 'c', 'a']\n",
      "['s', 'a', 'c', 'a', 's', 'd', 's']\n",
      "['s', 'a', 'c', 'a', 's', 'd', 's']\n",
      "['s', 'a', 'c', 'd', 's', 'a', 's']\n",
      "['s', 'a', 'c', 'd', 's', 'a', 's']\n",
      "['s', 'a', 'c', 's', 'a', 'd', 's']\n",
      "['s', 'a', 'c', 's', 'a', 's', 'd']\n",
      "['s', 'a', 'c', 's', 'd', 'a', 's']\n",
      "['s', 'a', 'c', 's', 'd', 's', 'a']\n",
      "['s', 'a', 'c', 's', 'a', 'd', 's']\n",
      "['s', 'a', 'c', 's', 'a', 's', 'd']\n",
      "['s', 'a', 'c', 's', 'd', 'a', 's']\n",
      "['s', 'a', 'c', 's', 'd', 's', 'a']\n",
      "['s', 'a', 'd', 'a', 's', 'c', 's']\n",
      "['s', 'a', 'd', 'a', 's', 'c', 's']\n",
      "['s', 'a', 'd', 'c', 's', 'a', 's']\n",
      "['s', 'a', 'd', 'c', 's', 'a', 's']\n",
      "['s', 'a', 'd', 's', 'a', 'c', 's']\n",
      "['s', 'a', 'd', 's', 'a', 's', 'c']\n",
      "['s', 'a', 'd', 's', 'c', 'a', 's']\n",
      "['s', 'a', 'd', 's', 'c', 's', 'a']\n",
      "['s', 'a', 'd', 's', 'a', 'c', 's']\n",
      "['s', 'a', 'd', 's', 'a', 's', 'c']\n",
      "['s', 'a', 'd', 's', 'c', 'a', 's']\n",
      "['s', 'a', 'd', 's', 'c', 's', 'a']\n",
      "['s', 'a', 's', 'a', 'c', 'd', 's']\n",
      "['s', 'a', 's', 'a', 'c', 's', 'd']\n",
      "['s', 'a', 's', 'a', 'd', 'c', 's']\n",
      "['s', 'a', 's', 'a', 'd', 's', 'c']\n",
      "['s', 'a', 's', 'a', 's', 'c', 'd']\n",
      "['s', 'a', 's', 'a', 's', 'd', 'c']\n",
      "['s', 'a', 's', 'c', 'a', 'd', 's']\n",
      "['s', 'a', 's', 'c', 'a', 's', 'd']\n",
      "['s', 'a', 's', 'c', 'd', 'a', 's']\n",
      "['s', 'a', 's', 'c', 'd', 's', 'a']\n",
      "['s', 'a', 's', 'c', 's', 'a', 'd']\n",
      "['s', 'a', 's', 'c', 's', 'd', 'a']\n",
      "['s', 'a', 's', 'd', 'a', 'c', 's']\n",
      "['s', 'a', 's', 'd', 'a', 's', 'c']\n",
      "['s', 'a', 's', 'd', 'c', 'a', 's']\n",
      "['s', 'a', 's', 'd', 'c', 's', 'a']\n",
      "['s', 'a', 's', 'd', 's', 'a', 'c']\n",
      "['s', 'a', 's', 'd', 's', 'c', 'a']\n",
      "['s', 'a', 's', 'a', 'c', 'd', 's']\n",
      "['s', 'a', 's', 'a', 'c', 's', 'd']\n",
      "['s', 'a', 's', 'a', 'd', 'c', 's']\n",
      "['s', 'a', 's', 'a', 'd', 's', 'c']\n",
      "['s', 'a', 's', 'a', 's', 'c', 'd']\n",
      "['s', 'a', 's', 'a', 's', 'd', 'c']\n",
      "['s', 'a', 's', 'c', 'a', 'd', 's']\n",
      "['s', 'a', 's', 'c', 'a', 's', 'd']\n",
      "['s', 'a', 's', 'c', 'd', 'a', 's']\n",
      "['s', 'a', 's', 'c', 'd', 's', 'a']\n",
      "['s', 'a', 's', 'c', 's', 'a', 'd']\n",
      "['s', 'a', 's', 'c', 's', 'd', 'a']\n",
      "['s', 'a', 's', 'd', 'a', 'c', 's']\n",
      "['s', 'a', 's', 'd', 'a', 's', 'c']\n",
      "['s', 'a', 's', 'd', 'c', 'a', 's']\n",
      "['s', 'a', 's', 'd', 'c', 's', 'a']\n",
      "['s', 'a', 's', 'd', 's', 'a', 'c']\n",
      "['s', 'a', 's', 'd', 's', 'c', 'a']\n",
      "['s', 'c', 'a', 'd', 's', 'a', 's']\n",
      "['s', 'c', 'a', 'd', 's', 'a', 's']\n",
      "['s', 'c', 'a', 's', 'a', 'd', 's']\n",
      "['s', 'c', 'a', 's', 'a', 's', 'd']\n",
      "['s', 'c', 'a', 's', 'd', 'a', 's']\n",
      "['s', 'c', 'a', 's', 'd', 's', 'a']\n",
      "['s', 'c', 'a', 's', 'a', 'd', 's']\n",
      "['s', 'c', 'a', 's', 'a', 's', 'd']\n",
      "['s', 'c', 'a', 's', 'd', 'a', 's']\n",
      "['s', 'c', 'a', 's', 'd', 's', 'a']\n",
      "['s', 'c', 'a', 'd', 's', 'a', 's']\n",
      "['s', 'c', 'a', 'd', 's', 'a', 's']\n",
      "['s', 'c', 'a', 's', 'a', 'd', 's']\n",
      "['s', 'c', 'a', 's', 'a', 's', 'd']\n",
      "['s', 'c', 'a', 's', 'd', 'a', 's']\n",
      "['s', 'c', 'a', 's', 'd', 's', 'a']\n",
      "['s', 'c', 'a', 's', 'a', 'd', 's']\n",
      "['s', 'c', 'a', 's', 'a', 's', 'd']\n",
      "['s', 'c', 'a', 's', 'd', 'a', 's']\n",
      "['s', 'c', 'a', 's', 'd', 's', 'a']\n",
      "['s', 'c', 'd', 'a', 's', 'a', 's']\n",
      "['s', 'c', 'd', 'a', 's', 'a', 's']\n",
      "['s', 'c', 'd', 'a', 's', 'a', 's']\n",
      "['s', 'c', 'd', 'a', 's', 'a', 's']\n",
      "['s', 'c', 'd', 's', 'a', 's', 'a']\n",
      "['s', 'c', 'd', 's', 'a', 's', 'a']\n",
      "['s', 'c', 'd', 's', 'a', 's', 'a']\n",
      "['s', 'c', 'd', 's', 'a', 's', 'a']\n",
      "['s', 'c', 's', 'a', 'd', 'a', 's']\n",
      "['s', 'c', 's', 'a', 'd', 's', 'a']\n",
      "['s', 'c', 's', 'a', 's', 'a', 'd']\n",
      "['s', 'c', 's', 'a', 's', 'd', 'a']\n",
      "['s', 'c', 's', 'a', 'd', 'a', 's']\n",
      "['s', 'c', 's', 'a', 'd', 's', 'a']\n",
      "['s', 'c', 's', 'a', 's', 'a', 'd']\n",
      "['s', 'c', 's', 'a', 's', 'd', 'a']\n",
      "['s', 'c', 's', 'd', 'a', 's', 'a']\n",
      "['s', 'c', 's', 'd', 'a', 's', 'a']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s', 'c', 's', 'a', 'd', 'a', 's']\n",
      "['s', 'c', 's', 'a', 'd', 's', 'a']\n",
      "['s', 'c', 's', 'a', 's', 'a', 'd']\n",
      "['s', 'c', 's', 'a', 's', 'd', 'a']\n",
      "['s', 'c', 's', 'a', 'd', 'a', 's']\n",
      "['s', 'c', 's', 'a', 'd', 's', 'a']\n",
      "['s', 'c', 's', 'a', 's', 'a', 'd']\n",
      "['s', 'c', 's', 'a', 's', 'd', 'a']\n",
      "['s', 'c', 's', 'd', 'a', 's', 'a']\n",
      "['s', 'c', 's', 'd', 'a', 's', 'a']\n",
      "['s', 'd', 'a', 'c', 's', 'a', 's']\n",
      "['s', 'd', 'a', 'c', 's', 'a', 's']\n",
      "['s', 'd', 'a', 's', 'a', 'c', 's']\n",
      "['s', 'd', 'a', 's', 'a', 's', 'c']\n",
      "['s', 'd', 'a', 's', 'c', 'a', 's']\n",
      "['s', 'd', 'a', 's', 'c', 's', 'a']\n",
      "['s', 'd', 'a', 's', 'a', 'c', 's']\n",
      "['s', 'd', 'a', 's', 'a', 's', 'c']\n",
      "['s', 'd', 'a', 's', 'c', 'a', 's']\n",
      "['s', 'd', 'a', 's', 'c', 's', 'a']\n",
      "['s', 'd', 'a', 'c', 's', 'a', 's']\n",
      "['s', 'd', 'a', 'c', 's', 'a', 's']\n",
      "['s', 'd', 'a', 's', 'a', 'c', 's']\n",
      "['s', 'd', 'a', 's', 'a', 's', 'c']\n",
      "['s', 'd', 'a', 's', 'c', 'a', 's']\n",
      "['s', 'd', 'a', 's', 'c', 's', 'a']\n",
      "['s', 'd', 'a', 's', 'a', 'c', 's']\n",
      "['s', 'd', 'a', 's', 'a', 's', 'c']\n",
      "['s', 'd', 'a', 's', 'c', 'a', 's']\n",
      "['s', 'd', 'a', 's', 'c', 's', 'a']\n",
      "['s', 'd', 'c', 'a', 's', 'a', 's']\n",
      "['s', 'd', 'c', 'a', 's', 'a', 's']\n",
      "['s', 'd', 'c', 'a', 's', 'a', 's']\n",
      "['s', 'd', 'c', 'a', 's', 'a', 's']\n",
      "['s', 'd', 'c', 's', 'a', 's', 'a']\n",
      "['s', 'd', 'c', 's', 'a', 's', 'a']\n",
      "['s', 'd', 'c', 's', 'a', 's', 'a']\n",
      "['s', 'd', 'c', 's', 'a', 's', 'a']\n",
      "['s', 'd', 's', 'a', 'c', 'a', 's']\n",
      "['s', 'd', 's', 'a', 'c', 's', 'a']\n",
      "['s', 'd', 's', 'a', 's', 'a', 'c']\n",
      "['s', 'd', 's', 'a', 's', 'c', 'a']\n",
      "['s', 'd', 's', 'a', 'c', 'a', 's']\n",
      "['s', 'd', 's', 'a', 'c', 's', 'a']\n",
      "['s', 'd', 's', 'a', 's', 'a', 'c']\n",
      "['s', 'd', 's', 'a', 's', 'c', 'a']\n",
      "['s', 'd', 's', 'c', 'a', 's', 'a']\n",
      "['s', 'd', 's', 'c', 'a', 's', 'a']\n",
      "['s', 'd', 's', 'a', 'c', 'a', 's']\n",
      "['s', 'd', 's', 'a', 'c', 's', 'a']\n",
      "['s', 'd', 's', 'a', 's', 'a', 'c']\n",
      "['s', 'd', 's', 'a', 's', 'c', 'a']\n",
      "['s', 'd', 's', 'a', 'c', 'a', 's']\n",
      "['s', 'd', 's', 'a', 'c', 's', 'a']\n",
      "['s', 'd', 's', 'a', 's', 'a', 'c']\n",
      "['s', 'd', 's', 'a', 's', 'c', 'a']\n",
      "['s', 'd', 's', 'c', 'a', 's', 'a']\n",
      "['s', 'd', 's', 'c', 'a', 's', 'a']\n"
     ]
    }
   ],
   "source": [
    "#in how many ways a string can be arranged so that no two adjacent chars are same \n",
    "string = \"aacdsss\"\n",
    "\n",
    "def per_nr(string, temp, index, j, final):\n",
    "    if j == len(string):\n",
    "        final.append(list(temp))\n",
    "        print(temp)\n",
    "        return\n",
    "    \n",
    "    for i in range(len(string)):\n",
    "        if i not in index:\n",
    "            if len(temp) == 0 or temp[-1] != string[i]:\n",
    "                temp.append(string[i])\n",
    "                index.append(i)\n",
    "                per_nr(string, temp, index, j+1, final)\n",
    "                temp.pop()\n",
    "                index.pop()\n",
    "            \n",
    "final = []\n",
    "per_nr(string, [], [], 0, final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 43, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = {1:3,2:3}\n",
    "l1 = [2,3,42,3]\n",
    "k = 0\n",
    "f = map(lambda x:x+1,l1)\n",
    "print(list(f))\n",
    "\n",
    "kf = lambda x:x+1\n",
    "kf(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6], [128], [60], [71], [10], [22], [0], [0], [0], [0]]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[1,2,33], [3,2,4,5,3,22], [1,124,23,66,7], [1]]\n",
    "\n",
    "def sum_index(a, ind, j, temp_sum, final, prev = [0]):\n",
    "    if j >= len(a):\n",
    "        final.append(list([temp_sum]))\n",
    "        ind[0] += 1\n",
    "        return\n",
    "    \n",
    "    if ind[0] < len(a[j]):\n",
    "        for index, i in enumerate(a[j]):\n",
    "            if index == ind[0]:\n",
    "                if prev[0] == ind[0] or temp_sum == 0:\n",
    "                    prev[0] = ind[0]\n",
    "                    temp_sum += i\n",
    "                    sum_index(a, ind, j+1, temp_sum, final, prev)\n",
    "                    temp_sum -= i\n",
    "    else:\n",
    "        sum_index(a, ind, j+1, temp_sum, final, prev)\n",
    "        \n",
    "    if temp_sum == 0:\n",
    "        sum_index(a, ind, j+1, temp_sum, final, prev)\n",
    "    \n",
    "ind = [0]\n",
    "final = []\n",
    "sum_index(a,ind,0,0,final)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 6, 1: 128, 2: 60, 3: 71, 4: 10, 5: 22}\n"
     ]
    }
   ],
   "source": [
    "#sum all elements in all the columns\n",
    "a = [[1,2,33], [3,2,4,5,3,22], [1,124,23,66,7], [1]]\n",
    "\n",
    "def sum_t(a, check, sm, l, r, l_in, r_in):\n",
    "    check[l][r] = True\n",
    "    if r in sm:\n",
    "        sm[r] += a[l][r]\n",
    "    else:\n",
    "        sm[r] = a[l][r]\n",
    "    \n",
    "    for i in range(4):\n",
    "        l1 = l+l_in[i]\n",
    "        r1 = r+r_in[i]\n",
    "        if l1 < len(a) and l1>=0 and r1<len(a[l1]) and r1>=0 and check[l1][r1] == False:\n",
    "            sum_t(a, check, sm, l1, r1, l_in, r_in)  \n",
    "    \n",
    "check = [[False for i in j] for j in a]\n",
    "\n",
    "sm = {}\n",
    "l_in = [1,0,-1,0]\n",
    "r_in = [0,1,0,-1]\n",
    "l = 0\n",
    "r = 0\n",
    "sum_t(a, check, sm, l, r, l_in, r_in)\n",
    "\n",
    "print(sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 4, 1: 2, 2: 11, 3: 3, 4: 3, 5: 1, 6: 1}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sum of all columns\n",
    "\n",
    "mat = [[1,1,1,0,0,1,1], [1,0,1,1,1], [1,0,0,1,1], [1,1,9,1,1]]\n",
    "\n",
    "la = [1,0,-1,0]\n",
    "ra = [0,1,0,-1]\n",
    "#if [l+la[i],r+ra[i]] not in path:  \n",
    "def path1(mat, l, r, visited, la, ra, sm):\n",
    "\n",
    "    if r in sm:\n",
    "        sm[r] += mat[l][r]\n",
    "    else:\n",
    "        sm[r] = mat[l][r]\n",
    "    visited[l][r] = True\n",
    "\n",
    "    for i in range(4):\n",
    "        if l+la[i] < len(mat) and r+ra[i] < len(mat[l+la[i]]) and l+la[i] >= 0 and r+ra[i] >= 0:\n",
    "            if visited[l+la[i]][r+ra[i]] == False:\n",
    "                path1(mat, l+la[i], r+ra[i], visited, la, ra, sm)\n",
    "#                 for tmp in temp_paths:\n",
    "#                     paths.append(tmp)   \n",
    "    \n",
    "    #visited[l][r] = False\n",
    "    #return paths\n",
    "    \n",
    "sm = {}\n",
    "visited = [[False for i in mat[0]] for j in mat]\n",
    "dist = 0\n",
    "path_final = [None]\n",
    "path1(mat, 0, 0, visited, la, ra, sm)\n",
    "\n",
    "sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-66-0e43cbc09461>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-66-0e43cbc09461>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "s1 = \"ABCDGH\"\n",
    "s2 = \"AEDFHR\"\n",
    "\n",
    "k = [0]*max(len(s1), len(s2))\n",
    "\n",
    "for i in range(len(s1)):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: 1, 3: 1, 4: 2, 21: 3, 22: 4, 25: 5, 11: 1, 5: 3} 5\n"
     ]
    }
   ],
   "source": [
    "#longest increasing subsequence count\n",
    "\n",
    "a = [11,3,4,21,22,5,2,25]\n",
    "b = {}\n",
    "b[a[0]] = 1\n",
    "mx = 0\n",
    "for i in range(len(a)-1):\n",
    "    j = a[i+1]\n",
    "    mx = 0\n",
    "\n",
    "    while j >= 0:\n",
    "        if j in b and mx < b[j]:\n",
    "            mx = b[j]\n",
    "        j-=1\n",
    "\n",
    "    b[a[i+1]] = 1+mx\n",
    "    \n",
    "mx = 0\n",
    "for i in b:\n",
    "    if b[i] > mx:\n",
    "        mx = b[i]\n",
    "print(b,mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'B', 'D']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#longest common repeatition subsequence\n",
    "l = ['A', 'A', 'B', 'E', 'B', 'C', 'D', 'D']\n",
    "\n",
    "s = {}\n",
    "final = []\n",
    "for i in l:\n",
    "    if i in s:\n",
    "        s[i] += 1\n",
    "        if s[i] == 2:\n",
    "            final.append(i)\n",
    "    else:\n",
    "        s[i] = 1\n",
    "        \n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#least number of numbers (powers of 5) that a bstring can have\n",
    "\n",
    "def power(out, x):\n",
    "    while out > 1:\n",
    "        out /= 5\n",
    "        \n",
    "    if out == 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "\n",
    "st = \"000101001\"\n",
    "len(st)\n",
    "\n",
    "sp_count = 0\n",
    "i = 0\n",
    "fin = []\n",
    "visit = [None]*(len(st)+1)\n",
    "visit[0] = 0\n",
    "for i in range(2, len(st)+1):\n",
    "    for j in range(0,i):\n",
    "        if visit[j] != None and power(int(st[j:i],2), 5):\n",
    "            if visit[i] == None:\n",
    "                visit[i] = visit[j]+1\n",
    "            else:\n",
    "                visit[i] = min(visit[i], visit[j]+1)\n",
    "            \n",
    "#use max instead of min if you want max number of elements\n",
    "            \n",
    "visit[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = \"F\"\n",
    "int(s1,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "power(0, 5)\n",
    "d = [[\"\"]]\n",
    "len(d[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[''],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [['0001']],\n",
       " [],\n",
       " [['000101'], ['0001', '01']],\n",
       " [],\n",
       " [],\n",
       " [['000101', '001'], ['0001', '01', '001']]]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using dynamic programming\n",
    "\n",
    "initial = [\"\"]\n",
    "dp = [[]]*(len(st)+1)\n",
    "dp[0] = initial;\n",
    "len(dp[0])\n",
    "st = \"000101001\"\n",
    "for i in range(2, len(st)+1):\n",
    "    lis = []\n",
    "    for j in range(0,i):\n",
    "        if len(dp[j]) > 0 and power(int(st[j:i],2), 5):\n",
    "            for s1 in dp[j]:\n",
    "                lis.append([st[j:i]] if s1==\"\"else s1+[st[j:i]])\n",
    "    \n",
    "    dp[i] = lis\n",
    "            \n",
    "dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "9\n",
      "1\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "#next farthest seat available from all the persons\n",
    "\n",
    "seats = [False]*10\n",
    "seats[7] = True\n",
    "\n",
    "def next_spot(seats):\n",
    "    ind = -1\n",
    "    occu = [i for i,b in enumerate(seats) if b == True]\n",
    "    mx = 0\n",
    "    for i, b in enumerate(seats):\n",
    "        if b == False:\n",
    "            diff = 0\n",
    "            for j in occu:\n",
    "                diff+=abs(i-j)\n",
    "                \n",
    "            if mx < diff:\n",
    "                mx = diff\n",
    "                ind = i\n",
    "           \n",
    "    seats[ind] = True\n",
    "    return ind\n",
    "\n",
    "print(next_spot(seats))\n",
    "print(next_spot(seats))\n",
    "print(next_spot(seats))\n",
    "print(next_spot(seats))\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#painting benches problem\n",
    "\n",
    "bench = [10,20,20,60]\n",
    "k = 3\n",
    "\n",
    "def time(bench, k):\n",
    "    tim = 0\n",
    "    sumb = sum(bench)\n",
    "    avg = sumb//k\n",
    "    i = 0\n",
    "    \n",
    "    tsum = 0\n",
    "    while i < len(bench):       \n",
    "        if tsum < avg:\n",
    "            tsum += bench[i]\n",
    "        \n",
    "        if tsum >= avg:\n",
    "            x = 0\n",
    "            if tsum-(bench[i]/2) > avg:\n",
    "                tsum-=bench[i]\n",
    "                #x = bench[i]\n",
    "                i-=1\n",
    "            \n",
    "            if tim < tsum:\n",
    "                tim = tsum\n",
    "                \n",
    "            \n",
    "            tsum = x\n",
    "            \n",
    "        i+=1\n",
    "\n",
    "    #if tim < tsum:\n",
    "    #    tim = tsum\n",
    "        \n",
    "    return tim\n",
    "\n",
    "\n",
    "time(bench, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
